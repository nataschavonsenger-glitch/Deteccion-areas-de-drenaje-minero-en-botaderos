{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69466ef-96b5-47f5-a9a7-b00650746c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, Iterable, Dict, List, Any\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\") \n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.lines import Line2D\n",
    "import gc\n",
    "\n",
    "BASE = Path(r\"C:\\Nuevos_excel\")\n",
    "CSV_PATH = BASE / \"Todo_10000.csv\"\n",
    "SEP = ';'\n",
    "OUT_DIR = BASE / \"mapas_interes_recurrencia_nv400\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATECOL = 'fecha'\n",
    "XCOL, YCOL = 'X', 'Y'\n",
    "\n",
    "KERNEL_X, KERNEL_Y = 400, 400     # tamaño del kernel \n",
    "MIN_PTS_KERNEL = 30               # mínimo de puntos en el kernel\n",
    "START_YEAR = 2019\n",
    "CHUNK = 1_000_000\n",
    "\n",
    "SAVE_DPI = 170\n",
    "PIL_KW = {\"quality\": 85, \"optimize\": True, \"progressive\": True}\n",
    "\n",
    "STREAMING_SAVE = False          \n",
    "KERNEL_LIMIT = None               \n",
    "MAX_BG_PTS = 250_000\n",
    "\n",
    "plt.rcParams['agg.path.chunksize'] = 20000\n",
    "plt.rcParams['path.simplify'] = True\n",
    "plt.rcParams['path.simplify_threshold'] = 0.5\n",
    "\n",
    "# Litología (predominio ≥60% reactivo vs caliza)\n",
    "REACTIVE_LITS = {'intrusivo', 'skarn'}\n",
    "CALIZA_LITS   = {'caliza', 'calizas', 'marmol', 'mármol', 'caliza/marmol', 'caliza/mármol'}\n",
    "LITO_MIN_FRAC = 0.60\n",
    "\n",
    "#  Umbrales humedad y Fe3\n",
    "NMDI_PCTL = 75   # percentil para humedad baja (NMDI ≤ P75)\n",
    "FE3_PCTL  = 80   # percentil para Fe3 alto   (Fe3 ≥ P80)\n",
    "\n",
    "\n",
    "FE2_DROP_MIN = 0.02  \n",
    "FE2_DROP_PCT = None   \n",
    "\n",
    "FE3_RISE_MIN = 0.05   \n",
    "FE3_RISE_PCT = None   \n",
    "\n",
    "USE_TEMP = False         \n",
    "TEMP_WINDOW_DAYS = 10     \n",
    "TEMP_DELTA_MIN = 1     \n",
    "RING_MARGIN_CELLS = 10    \n",
    "\n",
    "\n",
    "FIXED_COLOR_BY_COUNT = {\n",
    "    1: '#ff6ec7',  # rosado\n",
    "    2: '#00cfff',  # celeste\n",
    "    3: '#ffa600',  # naranjo\n",
    "    4: '#7e3ff2',  # morado\n",
    "    5: '#00d084',  # verde\n",
    "    6: '#ff4d4d',  # rojo\n",
    "    7: '#8dd3c7',\n",
    "    8: '#fb8072',\n",
    "    9: '#80b1d3',\n",
    "    10: '#fdb462',\n",
    "}\n",
    "def color_for_count(n: int):\n",
    "    if n in FIXED_COLOR_BY_COUNT:\n",
    "        return FIXED_COLOR_BY_COUNT[n]\n",
    "    cmap = plt.get_cmap('tab20')  # fallback estable\n",
    "    return cmap((n % 20) / 20.0)\n",
    "\n",
    "\n",
    "def BOOL(x): return str(x).strip().lower() in {'true','t','1','yes','y','si','sí'}\n",
    "\n",
    "def infer_grid_step(vals: np.ndarray) -> float:\n",
    "    u = np.sort(np.unique(vals[~pd.isna(vals)]))\n",
    "    if u.size <= 1: return 1.0\n",
    "    d = np.diff(u); d = d[(d > 0) & np.isfinite(d)]\n",
    "    return float(np.median(d)) if d.size else 1.0\n",
    "\n",
    "def build_grid_indices(df_pts: pd.DataFrame) -> Tuple[float, float]:\n",
    "    dx = infer_grid_step(df_pts[XCOL].values)\n",
    "    dy = infer_grid_step(df_pts[YCOL].values)\n",
    "    x0, y0 = df_pts[XCOL].min(), df_pts[YCOL].min()\n",
    "    df_pts['iX'] = np.round((df_pts[XCOL] - x0)/dx).astype('int32')\n",
    "    df_pts['iY'] = np.round((df_pts[YCOL] - y0)/dy).astype('int32')\n",
    "    return dx, dy\n",
    "\n",
    "def kernel_bbox(ix0, iy0, kx, ky):\n",
    "    hx, hy = (kx - 1)//2, (ky - 1)//2\n",
    "    return (ix0 - hx, ix0 + hx, iy0 - hy, iy0 + hy)\n",
    "\n",
    "def vecinos_de(ix0, iy0, kx, ky, cell2pids: Dict[tuple, List[int]]):\n",
    "    x0, x1, y0, y1 = kernel_bbox(ix0, iy0, kx, ky)\n",
    "    out = []\n",
    "    for ix in range(x0, x1 + 1):\n",
    "        for iy in range(y0, y1 + 1):\n",
    "            out.extend(cell2pids.get((ix, iy), []))\n",
    "    return list(set(out))\n",
    "\n",
    "def compute_temp_any_l8l9(df_: pd.DataFrame) -> pd.Series:\n",
    "    cols = []\n",
    "    if 'Temp_C_L9' in df_.columns:\n",
    "        m9 = df_.get('inside_L9', True)\n",
    "        t9 = pd.to_numeric(df_['Temp_C_L9'], errors='coerce')\n",
    "        v9 = np.where((m9 == True) & (t9 > -80.0), t9, np.nan)\n",
    "        cols.append(pd.Series(v9, index=df_.index, dtype='float32'))\n",
    "    if 'Temp_C_L8' in df_.columns:\n",
    "        m8 = df_.get('inside_L8', True)\n",
    "        t8 = pd.to_numeric(df_['Temp_C_L8'], errors='coerce')\n",
    "        v8 = np.where((m8 == True) & (t8 > -80.0), t8, np.nan)\n",
    "        cols.append(pd.Series(v8, index=df_.index, dtype='float32'))\n",
    "    if not cols:\n",
    "        return pd.Series(np.nan, index=df_.index, dtype='float32')\n",
    "    out = pd.concat(cols, axis=1).mean(axis=1, skipna=True).astype('float32')\n",
    "    out[out < -80.0] = np.nan\n",
    "    return out\n",
    "\n",
    "def nearest_mean_temp(df_region: pd.DataFrame, target_date: pd.Timestamp, max_days: int) -> float:\n",
    "    g = (df_region[['fecha_norm','Temp_any']]\n",
    "         .dropna()\n",
    "         .assign(fecha_norm=lambda d: pd.to_datetime(d['fecha_norm']).dt.normalize())\n",
    "         .groupby('fecha_norm')['Temp_any'].mean())\n",
    "    if g.empty:\n",
    "        return np.nan\n",
    "    dates_avail = pd.DatetimeIndex(g.index)\n",
    "    deltas = dates_avail - pd.Timestamp(target_date)  # TimedeltaIndex\n",
    "    td = pd.Timedelta(days=max_days)\n",
    "    mask = (deltas >= -td) & (deltas <= td)\n",
    "    if not mask.any():\n",
    "        return np.nan\n",
    "    filtered_dates = dates_avail[mask]\n",
    "    filtered_deltas_ns_abs = np.abs(deltas[mask].asi8)  # abs en ns\n",
    "    j = int(np.argmin(filtered_deltas_ns_abs))\n",
    "    closest_date = filtered_dates[j]\n",
    "    return float(g.loc[closest_date])\n",
    "\n",
    "NEEDED_COLS = [\n",
    "    'punto_id', DATECOL, XCOL, YCOL,\n",
    "    'inside_S2',\n",
    "    'B2_S2','B3_S2','B4_S2','B8A_S2','B11_S2','B12_S2',\n",
    "    'litologia',\n",
    "    'inside_L8','inside_L9','Temp_C_L8','Temp_C_L9'\n",
    "]\n",
    "DTYPES = {\n",
    "    XCOL:'float32', YCOL:'float32',\n",
    "    'B2_S2':'float32','B3_S2':'float32','B4_S2':'float32',\n",
    "    'B8A_S2':'float32','B11_S2':'float32','B12_S2':'float32',\n",
    "    'Temp_C_L8':'float32','Temp_C_L9':'float32'\n",
    "}\n",
    "\n",
    "rows = []\n",
    "need = set(NEEDED_COLS)\n",
    "\n",
    "it = pd.read_csv(\n",
    "    CSV_PATH, sep=SEP,\n",
    "    usecols=lambda c: c.strip() in need,\n",
    "    chunksize=CHUNK, engine=\"python\", on_bad_lines=\"skip\",\n",
    "    parse_dates=[DATECOL]\n",
    ")\n",
    "for i, ch in enumerate(it, 1):\n",
    "    ch.columns = ch.columns.str.strip()\n",
    "\n",
    "    for c, dt in DTYPES.items():\n",
    "        if c in ch.columns:\n",
    "            ch[c] = pd.to_numeric(ch[c], errors='coerce').astype(dt)\n",
    "\n",
    "    for b in ['inside_S2','inside_L8','inside_L9']:\n",
    "        if b in ch.columns and ch[b].dtype != bool:\n",
    "            ch[b] = ch[b].map(BOOL)\n",
    "\n",
    "    ch['punto_id'] = pd.to_numeric(ch['punto_id'], errors='coerce')\n",
    "    ch['punto_id'] = np.rint(ch['punto_id']).astype('Int64')\n",
    "\n",
    "    if DATECOL not in ch.columns:\n",
    "        print(f\"[WARN] Chunk {i} sin columna {DATECOL}; se omite.\")\n",
    "        continue\n",
    "    ch['fecha_norm'] = pd.to_datetime(ch[DATECOL], errors='coerce').dt.normalize()\n",
    "\n",
    "    if {'inside_S2','B2_S2','B3_S2','B4_S2','B8A_S2','B11_S2','B12_S2'}.issubset(ch.columns):\n",
    "        mS2 = (ch['inside_S2'] == True)\n",
    "        B2  = ch['B2_S2'].replace(0, np.nan)\n",
    "        B3  = ch['B3_S2'].replace(0, np.nan)\n",
    "        B4  = ch['B4_S2'].replace(0, np.nan)\n",
    "        B8A = ch['B8A_S2'].replace(0, np.nan)\n",
    "        B11 = ch['B11_S2'].replace(0, np.nan)\n",
    "        B12 = ch['B12_S2'].replace(0, np.nan)\n",
    "\n",
    "        ch['Ferric_minor_S2'] = np.where(mS2, B4 / B2, np.nan).astype('float32')                  # Fe3\n",
    "        ch['Ferrous_S2']      = np.where(mS2, (B3 + B11) / (B4 + B8A), np.nan).astype('float32')  # Fe2\n",
    "        denom = (B8A + (B11 - B12))\n",
    "        numer = (B8A - (B11 - B12))\n",
    "        nmdi = np.where(mS2 & np.isfinite(denom) & (np.abs(denom) > 1e-9), numer / denom, np.nan)\n",
    "        ch['NMDI_S2'] = pd.Series(nmdi, index=ch.index, dtype='float32')\n",
    "    else:\n",
    "        ch['Ferric_minor_S2'] = np.nan; ch['Ferrous_S2'] = np.nan; ch['NMDI_S2'] = np.nan\n",
    "\n",
    "    ch['Temp_any'] = compute_temp_any_l8l9(ch)\n",
    "\n",
    "    if 'litologia' in ch.columns:\n",
    "        ch['litologia'] = ch['litologia'].astype('category')\n",
    "\n",
    "    keep = ['punto_id','fecha_norm', XCOL, YCOL,\n",
    "            'Ferric_minor_S2','Ferrous_S2','NMDI_S2',\n",
    "            'inside_S2','litologia',\n",
    "            'inside_L8','inside_L9','Temp_C_L8','Temp_C_L9','Temp_any']\n",
    "    rows.append(ch[keep].copy())\n",
    "    print(f\"[LOAD] chunk {i} ({len(ch)})\")\n",
    "    if i % 5 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "if not rows:\n",
    "    raise SystemExit(\"No se pudo leer ningún chunk válido.\")\n",
    "df = pd.concat(rows, ignore_index=True)\n",
    "del rows; gc.collect()\n",
    "\n",
    "df = df[df['fecha_norm'] >= pd.Timestamp(START_YEAR, 1, 1)]\n",
    "if df.empty:\n",
    "    raise SystemExit(\"No hay datos después de START_YEAR.\")\n",
    "\n",
    "df_valid = df[(df['inside_S2']==True) & df['NMDI_S2'].notna() & df['Ferric_minor_S2'].notna()].copy()\n",
    "\n",
    "def q(arr, p, fb):\n",
    "    return float(np.nanpercentile(arr, p)) if arr.size else fb\n",
    "\n",
    "NMDI_Pth = q(df_valid['NMDI_S2'].to_numpy('float64'), NMDI_PCTL, 0.91)  # fallback típico ~P75\n",
    "FE3_Pth  = q(df_valid['Ferric_minor_S2'].to_numpy('float64'), FE3_PCTL, 1.20)\n",
    "\n",
    "(OUT_DIR / \"_umbrales_y_parametros.txt\").write_text(\n",
    "    \"Umbrales (inside_S2==True)\\n\"\n",
    "    f\"NMDI P{NMDI_PCTL}: {NMDI_Pth:.4f}  |  Fe³⁺ P{FE3_PCTL}: {FE3_Pth:.4f}\\n\"\n",
    "    \"Parámetros Cond. B (Fe2↓ y Fe3↑):\\n\"\n",
    "    f\"  FE2_DROP_MIN: {FE2_DROP_MIN} | FE2_DROP_PCT: {FE2_DROP_PCT}\\n\"\n",
    "    f\"  FE3_RISE_MIN: {FE3_RISE_MIN} | FE3_RISE_PCT: {FE3_RISE_PCT}\\n\"\n",
    "    \"Filtro térmico:\\n\"\n",
    "    f\"  USE_TEMP: {USE_TEMP} | TEMP_WINDOW_DAYS: {TEMP_WINDOW_DAYS} | \"\n",
    "    f\"TEMP_DELTA_MIN: {TEMP_DELTA_MIN} | RING_MARGIN_CELLS: {RING_MARGIN_CELLS}\\n\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "df_pts = (df[['punto_id', XCOL, YCOL]].dropna()\n",
    "          .drop_duplicates('punto_id').reset_index(drop=True))\n",
    "dx, dy = build_grid_indices(df_pts)\n",
    "df = df.merge(df_pts[['punto_id','iX','iY']], on='punto_id', how='left')\n",
    "\n",
    "# Un centro por celda (evita duplicar kernels)\n",
    "df_centros = (df[['iX','iY','punto_id']].dropna()\n",
    "              .drop_duplicates(subset=['iX','iY'], keep='first'))\n",
    "centros = (df_centros.astype({'punto_id':'int64'})\n",
    "           .sort_values(['iX','iY']).values.tolist())\n",
    "\n",
    "cell2pids = df_pts.groupby(['iX','iY'])['punto_id'].apply(list).to_dict()\n",
    "\n",
    "MAP_DIR = OUT_DIR / \"mapas_interes_por_fecha\"\n",
    "MAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lit_map = (\n",
    "    df[['punto_id', 'litologia']].dropna()\n",
    "      .assign(litologia=lambda d: d['litologia'].astype(str).str.strip().str.lower())\n",
    "      .groupby('punto_id')['litologia']\n",
    "      .agg(lambda s: s.mode().iloc[0] if not s.empty else np.nan)\n",
    "      .to_dict()\n",
    ")\n",
    "def kernel_is_reactive_vs_caliza(pids, min_frac=LITO_MIN_FRAC):\n",
    "    if not pids:\n",
    "        return False\n",
    "    lits = [lit_map.get(int(pid)) for pid in pids]\n",
    "    lits_known = [l for l in lits if isinstance(l, str) and l and l != 'nan']\n",
    "    if not lits_known:\n",
    "        return False\n",
    "    n_reac = sum(l in REACTIVE_LITS for l in lits_known)\n",
    "    n_calc = sum(l in CALIZA_LITS   for l in lits_known)\n",
    "    den = n_reac + n_calc\n",
    "    if den == 0:\n",
    "        return False\n",
    "    return (n_reac / den) >= min_frac\n",
    "\n",
    "candidates_by_kernel: Dict[tuple, Dict[pd.Timestamp, tuple]] = defaultdict(dict)\n",
    "\n",
    "for idx, (ix0, iy0, pid0) in enumerate(centros, 1):\n",
    "    if KERNEL_LIMIT is not None and idx > int(KERNEL_LIMIT):\n",
    "        break\n",
    "\n",
    "    # Kernel fijo (NO expandir)\n",
    "    vec_in = vecinos_de(int(ix0), int(iy0), KERNEL_X, KERNEL_Y, cell2pids)\n",
    "    if len(vec_in) < MIN_PTS_KERNEL:\n",
    "        continue \n",
    "\n",
    "    if not kernel_is_reactive_vs_caliza(vec_in):\n",
    "        continue\n",
    "\n",
    "    kdf = df[(df['punto_id'].isin(vec_in)) & (df['inside_S2'] == True)].copy()\n",
    "    kdf = kdf[kdf[['Ferric_minor_S2','Ferrous_S2','NMDI_S2']].notna().all(axis=1)]\n",
    "    if kdf.empty:\n",
    "        continue\n",
    "        \n",
    "    per_date = (\n",
    "        kdf.groupby('fecha_norm', as_index=False)\n",
    "           .agg(Ferric_mean=('Ferric_minor_S2','mean'),\n",
    "                NMDI_mean=('NMDI_S2','mean'),\n",
    "                Ferrous_mean=('Ferrous_S2','mean'))\n",
    "           .sort_values('fecha_norm')\n",
    "           .reset_index(drop=True)\n",
    "    )\n",
    "    if per_date.empty:\n",
    "        continue\n",
    "    per_date['cond_A'] = (per_date['Ferric_mean'] >= FE3_Pth) & (per_date['NMDI_mean'] <= NMDI_Pth)\n",
    "    per_date['Ferrous_prev'] = per_date['Ferrous_mean'].shift(1)\n",
    "    per_date['Ferric_prev']  = per_date['Ferric_mean'].shift(1)\n",
    "\n",
    "    if FE2_DROP_PCT is not None:\n",
    "        cond_fe2 = (\n",
    "            per_date['Ferrous_prev'].notna() &\n",
    "            ((per_date['Ferrous_prev'] - per_date['Ferrous_mean']) / per_date['Ferrous_prev'] >= FE2_DROP_PCT)\n",
    "        )\n",
    "    else:\n",
    "        cond_fe2 = (\n",
    "            per_date['Ferrous_prev'].notna() &\n",
    "            ((per_date['Ferrous_prev'] - per_date['Ferrous_mean']) >= FE2_DROP_MIN)\n",
    "        )\n",
    "\n",
    "    if FE3_RISE_PCT is not None:\n",
    "        cond_fe3 = (\n",
    "            per_date['Ferric_prev'].notna() &\n",
    "            ((per_date['Ferric_mean'] - per_date['Ferric_prev']) / per_date['Ferric_prev'] >= FE3_RISE_PCT)\n",
    "        )\n",
    "    else:\n",
    "        cond_fe3 = (\n",
    "            per_date['Ferric_prev'].notna() &\n",
    "            ((per_date['Ferric_mean'] - per_date['Ferric_prev']) >= FE3_RISE_MIN)\n",
    "        )\n",
    "\n",
    "    per_date['cond_B'] = (per_date['Ferric_mean'] >= FE3_Pth) & cond_fe2 & cond_fe3\n",
    "\n",
    "    cx0 = df_pts.loc[df_pts['punto_id'] == pid0, XCOL].iloc[0]\n",
    "    cy0 = df_pts.loc[df_pts['punto_id'] == pid0, YCOL].iloc[0]\n",
    "    hx, hy = (KERNEL_X - 1)//2, (KERNEL_Y - 1)//2\n",
    "    x0r, x1r = cx0 - hx*dx, cx0 + hx*dx\n",
    "    y0r, y1r = cy0 - hy*dy, cy0 + hy*dy\n",
    "\n",
    "    key = (int(ix0), int(iy0))  \n",
    "\n",
    "    for _, r in per_date.iterrows():\n",
    "        if bool(r['cond_A']) and bool(r['cond_B']):\n",
    "            d = pd.to_datetime(r['fecha_norm']).normalize()\n",
    "\n",
    "            if USE_TEMP:\n",
    "                vec_ring_all = vecinos_de(int(ix0), int(iy0),\n",
    "                                          KERNEL_X + 2*RING_MARGIN_CELLS,\n",
    "                                          KERNEL_Y + 2*RING_MARGIN_CELLS,\n",
    "                                          cell2pids)\n",
    "                vec_ring = list(set(vec_ring_all) - set(vec_in))\n",
    "                if not vec_ring:\n",
    "                    continue\n",
    "\n",
    "                kdf_temp = df[df['punto_id'].isin(vec_in)][['fecha_norm','Temp_any']]\n",
    "                rdf_temp = df[df['punto_id'].isin(vec_ring)][['fecha_norm','Temp_any']]\n",
    "\n",
    "                tk = nearest_mean_temp(kdf_temp, d, TEMP_WINDOW_DAYS)\n",
    "                tr = nearest_mean_temp(rdf_temp, d, TEMP_WINDOW_DAYS)\n",
    "\n",
    "                if np.isnan(tk) or np.isnan(tr) or (tk - tr) < TEMP_DELTA_MIN:\n",
    "                    continue  \n",
    "            candidates_by_kernel[key][d] = (x0r, y0r, x1r, y1r)\n",
    "\n",
    "recurrence_by_kernel: Dict[tuple, int] = {\n",
    "    key: len(dates_dict) for key, dates_dict in candidates_by_kernel.items() if dates_dict\n",
    "}\n",
    "\n",
    "interest_by_date: Dict[pd.Timestamp, List[Dict[str, Any]]] = defaultdict(list)\n",
    "for key, dates_dict in candidates_by_kernel.items():\n",
    "    if not dates_dict:\n",
    "        continue\n",
    "    count = recurrence_by_kernel.get(key, 0)\n",
    "    if count <= 2:\n",
    "        continue\n",
    "    for d, bbox in dates_dict.items():\n",
    "        x0r, y0r, x1r, y1r = bbox\n",
    "        interest_by_date[d].append({\n",
    "            'key': key,\n",
    "            'x0': x0r, 'y0': y0r, 'x1': x1r, 'y1': y1r,\n",
    "            'count': count\n",
    "        })\n",
    "\n",
    "if not interest_by_date:\n",
    "    print(\"[INFO] No hay fechas con zonas A∩B para exportar.\")\n",
    "else:\n",
    "    for d in sorted(interest_by_date.keys()):\n",
    "        rects = interest_by_date[d]\n",
    "        if not rects:\n",
    "            continue\n",
    "        out_img = MAP_DIR / f\"map_interes_rec_{pd.Timestamp(d):%Y%m%d}.jpg\"\n",
    "\n",
    "        pts_ids = df.loc[(df['fecha_norm'] == d) & (df['inside_S2'] == True), 'punto_id']\n",
    "        pts_coords = df_pts[df_pts['punto_id'].isin(pts_ids)].copy()\n",
    "        if len(pts_coords) > MAX_BG_PTS:\n",
    "            pts_coords = pts_coords.sample(MAX_BG_PTS, random_state=0)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(14, 12))\n",
    "        if not pts_coords.empty:\n",
    "            ax.scatter(pts_coords[XCOL], pts_coords[YCOL], s=2, color='#bdbdbd', alpha=0.6, zorder=0)\n",
    "            xmin, xmax = pts_coords[XCOL].min(), pts_coords[XCOL].max()\n",
    "            ymin, ymax = pts_coords[YCOL].min(), pts_coords[YCOL].max()\n",
    "            rx = max(xmax - xmin, dx); ry = max(ymax - ymin, dy)\n",
    "            ax.set_xlim(xmin - 0.05*rx, xmax + 0.05*rx)\n",
    "            ax.set_ylim(ymin - 0.05*ry, ymax + 0.05*ry)\n",
    "\n",
    "        counts_present = []\n",
    "        for it in rects:\n",
    "            c = int(it['count'])\n",
    "            counts_present.append(c)\n",
    "            color = color_for_count(c)\n",
    "            rect = patches.Rectangle(\n",
    "                (it['x0'], it['y0']),\n",
    "                it['x1'] - it['x0'], it['y1'] - it['y0'],\n",
    "                fill=False, lw=2.0, ec=color, alpha=0.95\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "        uniq_counts = sorted(set(counts_present))\n",
    "        legend_elems = [Line2D([0], [0], color=color_for_count(c), lw=2,\n",
    "                        label=f\"Recurrencia = {c}\") for c in uniq_counts]\n",
    "        ax.legend(handles=legend_elems, loc='upper left', bbox_to_anchor=(1.02, 1.0),\n",
    "                  borderaxespad=0., frameon=True, title=\"A∩B (Fe³⁺ alto, NMDI bajo, Fe²⁺↓ y Fe³⁺↑)\")\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"Zonas A∩B coloreadas por recurrencia total del kernel (sin expansión)\\n\"\n",
    "            f\"(inside_S2==True; Lito Intrusivo/Skarn ≥60% vs Caliza) · {pd.Timestamp(d):%Y-%m-%d}\",\n",
    "            fontsize=11\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            fig.savefig(out_img, dpi=SAVE_DPI, facecolor='white',\n",
    "                        bbox_inches='tight', pil_kwargs=PIL_KW)\n",
    "        except TypeError:\n",
    "            fig.savefig(out_img, dpi=SAVE_DPI, facecolor='white', bbox_inches='tight')\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR savefig] {out_img}: {e}\")\n",
    "        else:\n",
    "            print(f\"[SAVE] {out_img} (rects={len(rects)})\")\n",
    "        plt.close(fig)\n",
    "\n",
    "print(f\"[OK] Mapas por fecha → {MAP_DIR}\")\n",
    "\n",
    "# Limpieza\n",
    "del df, df_pts, cell2pids\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
