{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff765a-ead0-4db8-87fb-108a57ea2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentinel-1_cambios por punto de la malla\n",
    "\n",
    "import ee\n",
    "ee.Initialize()\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy \n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, gamma, f, chi2, lognorm, kstest,fisk\n",
    "import pandas as pd\n",
    "import folium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import math\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from IPython.display import Image\n",
    "import IPython.display as disp\n",
    "import imageio\n",
    "#%matplotlib inline\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from openpyxl import load_workbook\n",
    "import scipy.stats as st\n",
    "from typing import Optional\n",
    "import datetime as dt\n",
    "import re\n",
    "from pathlib import Path\n",
    "from openpyxl import load_workbook\n",
    "from bisect import bisect_left\n",
    "\n",
    "def append_df_to_excel(filename, df, sheet_name, startrow=None,truncate_sheet=False,**to_excel_kwargs):\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        df.to_excel(\n",
    "            filename,\n",
    "            sheet_name=sheet_name, \n",
    "            startrow=startrow if startrow is not None else 0, \n",
    "            **to_excel_kwargs)\n",
    "        return\n",
    "    \n",
    "    if 'engine' in to_excel_kwargs:\n",
    "        to_excel_kwargs.pop('engine')\n",
    "    writer = pd.ExcelWriter(filename, engine='openpyxl', mode='a')\n",
    "    writer.book = load_workbook(filename)\n",
    "    \n",
    "    if startrow is None and sheet_name in writer.book.sheetnames:\n",
    "        startrow = writer.book[sheet_name].max_row\n",
    "\n",
    "    if truncate_sheet and sheet_name in writer.book.sheetnames:\n",
    "        idx = writer.book.sheetnames.index(sheet_name)\n",
    "        writer.book.remove(writer.book.worksheets[idx])\n",
    "        writer.book.create_sheet(sheet_name, idx)\n",
    "    \n",
    "    writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n",
    "\n",
    "    if startrow is None:\n",
    "        startrow = 0\n",
    "    df.to_excel(writer, sheet_name, startrow=startrow, **to_excel_kwargs)\n",
    "    writer.save()\n",
    "\n",
    "from scipy.stats import (\n",
    "    norm, beta, expon, gamma, genextreme, logistic, lognorm, triang, uniform, fatiguelife,            \n",
    "    gengamma, gennorm, dweibull, dgamma, gumbel_r, powernorm, rayleigh, weibull_max, weibull_min, \n",
    "    laplace, alpha, genexpon, bradford, betaprime, burr, fisk, f, genpareto, hypsecant, \n",
    "    halfnorm, halflogistic, invgauss, invgamma, levy, loglaplace, loggamma, maxwell, \n",
    "    mielke, ncx2, ncf, nct, nakagami, pareto, lomax, powerlognorm, powerlaw, rice, \n",
    "    semicircular, rice, invweibull, foldnorm, foldcauchy, cosine, exponpow, \n",
    "    exponweib, wald, wrapcauchy, truncexpon, truncnorm, t, rdist\n",
    "    )\n",
    "\n",
    "distributions = [\n",
    "    norm, beta, expon, gamma, genextreme, logistic, lognorm, triang, uniform, fatiguelife,            \n",
    "    gengamma, gennorm, dweibull, dgamma, gumbel_r, powernorm, rayleigh, weibull_max, weibull_min, \n",
    "    laplace, alpha, genexpon, bradford, betaprime, burr, fisk, f, genpareto, hypsecant, \n",
    "    halfnorm, halflogistic, invgauss, invgamma, levy, loglaplace, loggamma, maxwell, \n",
    "    mielke, ncx2, ncf, nct, nakagami, pareto, lomax, powerlognorm, powerlaw, rice, \n",
    "    semicircular, rice, invweibull, foldnorm, foldcauchy, cosine, exponpow, \n",
    "    exponweib, wald, wrapcauchy, truncexpon, truncnorm, t, rdist\n",
    "    ]\n",
    "\n",
    "dist_continu = [d for d in dir(stats) if\n",
    "                isinstance(getattr(stats, d), stats.rv_continuous)]\n",
    "dist_discrete = [d for d in dir(stats) if\n",
    "                 isinstance(getattr(stats, d), stats.rv_discrete)]\n",
    "print('number of continuous distributions: %d' % len(dist_continu))\n",
    "print('number of discrete distributions:   %d' % len(dist_discrete))\n",
    "\n",
    "# función de folium \n",
    "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
    "  map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
    "  folium.raster_layers.TileLayer(\n",
    "    tiles = map_id_dict['tile_fetcher'].url_format,\n",
    "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "    name = name,\n",
    "    overlay = True,\n",
    "    control = True\n",
    "  ).add_to(self)\n",
    "  \n",
    "# función de distribución chi2 para serie temporal.\n",
    "def chi2cdf(chi2, df):\n",
    "    \"\"\"Calculates Chi square cumulative distribution function for\n",
    "       df degrees of freedom using the built-in incomplete gamma\n",
    "       function gammainc().\n",
    "    \"\"\"\n",
    "    return ee.Image(chi2.divide(2)).gammainc(ee.Number(df).divide(2))\n",
    "\n",
    "def det(im):\n",
    "    \"\"\"Calculates determinant of 2x2 diagonal covariance matrix.\"\"\"\n",
    "    return im.expression('b(0)*b(1)')\n",
    "\n",
    "#### Función ks-test para obtener el valor de P con el dataframe, ver df\n",
    "def kstest(data, distname, paramtup):\n",
    "    ksN = 100           # Kolmogorov-Smirnov KS test for goodness of fit: samples    \n",
    "    ks = scipy.stats.kstest(data, distname, paramtup, ksN)[1] # return p-value\n",
    "    return ks             # return p-value\n",
    "# distribution fitter and call to KS test\n",
    "def fitdist(data1, dist):    \n",
    "    fitted = dist.fit(data1, floc=0.0)\n",
    "    ks = kstest(data1, dist.name, fitted)\n",
    "    res = (dist.name, ks , *fitted)\n",
    "    return res\n",
    "\n",
    "# histogramas obtenidos para las distribuciones con valor P > alpha \n",
    "def plot_fitted_pdf(df):\n",
    "    \n",
    "    N = len(df)\n",
    "    chrows = math.ceil(N/3) # how many rows of charts if 3 in a row\n",
    "    fig, ax = plt.subplots(chrows, 3, figsize=(20, 5 * chrows))\n",
    "    ax = ax.ravel()\n",
    "    dfRV = pd.DataFrame()\n",
    "    for i in df.index:\n",
    "        # D_row = df.iloc[i,:-1]\n",
    "        D_name = df.iloc[i,0]\n",
    "        D = df.iloc[i,7]\n",
    "        KSp = df.iloc[i,1]\n",
    "        params = df.iloc[i,2:7]    \n",
    "        params = [p for p in params if ~np.isnan(p)]\n",
    "# calibrate x-axis by finding the 1% and 99% quantiles in percent point function\n",
    "        x = np.linspace(\n",
    "                    D.ppf(0.01, *params), \n",
    "                    D.ppf(0.99, *params), 100)\n",
    "        #fig, ax = plt.subplots(1, 1)\n",
    "        # plot histogram of actual observations\n",
    "        ax[i].hist(df, density=True, histtype='stepfilled', alpha=0.2)\n",
    "        # plot fitted distribution\n",
    "        rv = D(*params)\n",
    "        title = f'pdf {D_name}, with p(KS): {KSp:.2f}' \n",
    "        ax[i].plot(x, rv.pdf(x), 'r-', lw=2, label=title)\n",
    "        ax[i].legend(loc=\"upper right\", frameon=False)  \n",
    "\n",
    "def getCols(tableMetadata):\n",
    "    return tableMetadata.columns\n",
    "\n",
    "# This function adds a band representing the image timestamp.\n",
    "def addTime(image): \n",
    "  return image.addBands(image.metadata('system:time_start'))\n",
    "\n",
    "\n",
    "def coverage_filter(geom,resolution,area,threshold,list_bands):\n",
    "  \n",
    "  def MapImageCol (single_image):\n",
    "    actual_cover_area=(single_image.select(list_bands).clip(geom).reduceRegion(**{'reducer': ee.Reducer.count(),'maxPixels': 1E13}).values().get(0))\n",
    "    gee_num=ee.Number(actual_cover_area).multiply(resolution).divide(area)\n",
    "\n",
    "    return ee.Algorithms.If(gee_num.gt(threshold),single_image,ee.Image(0))\n",
    "\n",
    "  return MapImageCol\n",
    "\n",
    "def RGB_images(satellite,geom,scene_collection,filepath):             \n",
    "    acq_times1 = scene_collection.aggregate_array('system:time_start').getInfo()\n",
    "    dates1=[time.strftime('%x', time.gmtime(acq_time/1000)) for acq_time in acq_times1]  \n",
    "    \n",
    "    # Selección de dos imagenes y extracción de las bandas VV\n",
    "    scene_list = scene_collection.toList(scene_collection.size())\n",
    "    \n",
    "    if scene_collection.size().getInfo()==0:\n",
    "        print('no scenes available for images generation')        \n",
    "        return [0,'','','','']\n",
    "    \n",
    "    elif scene_collection.size().getInfo()==1:\n",
    "        print('1 scene available')\n",
    "        \n",
    "        im1 = ee.Image(scene_list.get(0))\n",
    "        \n",
    "        if satellite in ['L8']:\n",
    "            hsv = im1.select(['B4', 'B3', 'B2']).rgbToHsv()\n",
    "            sharpened = ee.Image.cat([hsv.select('hue'), hsv.select('saturation'), im1.select('B8')]).hsvToRgb()\n",
    "            image_rgb1 = sharpened.visualize(**{'bands': ['red', 'green', 'blue'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "    \n",
    "        if satellite in ['L7']:\n",
    "            hsv = im1.select(['B3', 'B2','B1']).rgbToHsv()\n",
    "            sharpened = ee.Image.cat([hsv.select('hue'), hsv.select('saturation'), im1.select('B8')]).hsvToRgb()\n",
    "            image_rgb1 = sharpened.visualize(**{'bands': ['red', 'green', 'blue'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "    \n",
    "    \n",
    "        if satellite in ['S2']:\n",
    "            rgb = im1.select(['B4', 'B3', 'B2']).divide(10000)\n",
    "            image_rgb1 = rgb.visualize(**{'bands': ['B4', 'B3', 'B2'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "    \n",
    "        if satellite in ['L4','L5']:\n",
    "            rgb = im1.select(['B3', 'B2', 'B1'])\n",
    "            image_rgb1 = rgb.visualize(**{'bands': ['B3', 'B2', 'B1'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "    \n",
    "        \n",
    "        display4 = ee.Image(0).updateMask(0).paint(mined_area,'red',2)\n",
    "        image_vect_distrubed_zones= display4.visualize(**{'min': 0, 'max': 20, 'palette': ['black', 'black']})    \n",
    "        \n",
    "        mosaic1 = ee.ImageCollection([image_rgb1,image_vect_distrubed_zones]).mosaic()\n",
    "           \n",
    "        # Create a URL to the styled image for a region around France.\n",
    "        url = mosaic1.getThumbUrl({'dimensions': [1024,768] , 'region': geom})\n",
    "                \n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        driver.get(url)\n",
    "        \n",
    "        try:\n",
    "          os.makedirs(filepath + '\\\\' + satellite + '\\\\')\n",
    "        except:\n",
    "          print (\"folder already created\")\n",
    "    \n",
    "        driver.get_screenshot_as_file(filepath + '\\\\' + satellite + '\\\\' +'past_image.png')\n",
    "        driver.quit()\n",
    "             \n",
    "        return [1,filepath + '\\\\' + satellite + '\\\\' +'past_image.png', dates1[0],'','']\n",
    "       \n",
    "    else:\n",
    "            \n",
    "        index1 = 0\n",
    "        index2 = scene_collection.size().getInfo()-1\n",
    "        \n",
    "        #past#\n",
    "        im1 = ee.Image(scene_list.get(index1))\n",
    "       \n",
    "        #present#\n",
    "        im2 = ee.Image(scene_list.get(index2))        \n",
    "    \n",
    "        \n",
    "        if satellite in ['L8']:\n",
    "            hsv = im1.select(['B4', 'B3', 'B2']).rgbToHsv()\n",
    "            sharpened = ee.Image.cat([hsv.select('hue'), hsv.select('saturation'), im1.select('B8')]).hsvToRgb()\n",
    "            image_rgb1 = sharpened.visualize(**{'bands': ['red', 'green', 'blue'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "    \n",
    "            hsv = im2.select(['B4', 'B3', 'B2']).rgbToHsv()\n",
    "            sharpened = ee.Image.cat([hsv.select('hue'), hsv.select('saturation'), im2.select('B8')]).hsvToRgb()\n",
    "            image_rgb2 = sharpened.visualize(**{'bands': ['red', 'green', 'blue'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "    \n",
    "        if satellite in ['L7']:\n",
    "            hsv = im1.select(['B3', 'B2','B1']).rgbToHsv()\n",
    "            sharpened = ee.Image.cat([hsv.select('hue'), hsv.select('saturation'), im1.select('B8')]).hsvToRgb()\n",
    "            image_rgb1 = sharpened.visualize(**{'bands': ['red', 'green', 'blue'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "    \n",
    "            hsv = im2.select(['B3', 'B2','B1']).rgbToHsv()\n",
    "            sharpened = ee.Image.cat([hsv.select('hue'), hsv.select('saturation'), im2.select('B8')]).hsvToRgb()\n",
    "            image_rgb2 = sharpened.visualize(**{'bands': ['red', 'green', 'blue'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "    \n",
    "    \n",
    "        if satellite in ['S2']:\n",
    "            rgb = im1.select(['B4', 'B3', 'B2']).divide(10000)\n",
    "            image_rgb1 = rgb.visualize(**{'bands': ['B4', 'B3', 'B2'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "    \n",
    "            rgb = im2.select(['B4', 'B3', 'B2']).divide(10000)\n",
    "            image_rgb2 = rgb.visualize(**{'bands': ['B4', 'B3', 'B2'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "        \n",
    "       \n",
    "        if satellite in ['L4','L5']:\n",
    "            rgb = im1.select(['B3', 'B2', 'B1'])\n",
    "            image_rgb1 = rgb.visualize(**{'bands': ['B3', 'B2', 'B1'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "    \n",
    "            rgb = im2.select(['B3', 'B2', 'B1'])\n",
    "            image_rgb2 = rgb.visualize(**{'bands': ['B3', 'B2', 'B1'],'min': 0, 'max': 0.25, 'gamma': [1.1, 1.1, 1]})\n",
    "      \n",
    "        \n",
    "        display4 = ee.Image(0).updateMask(0).paint(mined_area,'red',2)\n",
    "        image_vect_distrubed_zones= display4.visualize(**{'min': 0, 'max': 20, 'palette': ['black', 'black']})    \n",
    "        \n",
    "        mosaic1 = ee.ImageCollection([image_rgb1,image_vect_distrubed_zones]).mosaic()\n",
    "        mosaic2 = ee.ImageCollection([image_rgb2,image_vect_distrubed_zones]).mosaic()\n",
    "           \n",
    "        # Create a URL to the styled image for a region around France.\n",
    "        url = mosaic1.getThumbUrl({'dimensions': [1024,768] , 'region': geom})\n",
    "                \n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "          os.makedirs(filepath + '\\\\' + satellite + '\\\\')\n",
    "        except:\n",
    "          print (\"folder already created\")\n",
    "    \n",
    "        driver.get_screenshot_as_file(filepath + '\\\\' + satellite + '\\\\' +'past_image.png')\n",
    "        driver.quit()\n",
    "                   \n",
    "        # Create a URL to the styled image for a region around France.\n",
    "        url = mosaic2.getThumbUrl({'dimensions': [1024,768] , 'region': geom})\n",
    "                \n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        driver.get(url)\n",
    "    \n",
    "        driver.get_screenshot_as_file(filepath + '\\\\' + satellite + '\\\\'  +'present_image.png')\n",
    "        driver.quit()\n",
    "    \n",
    "        \n",
    "        return [2,filepath + '\\\\' + satellite + '\\\\' +'past_image.png', dates1[0], filepath + '\\\\' + satellite + '\\\\'  +'present_image.png',dates1[index2]]\n",
    "\n",
    "def _fecha_de_nombre(zona_id: str) -> dt.date:\n",
    "    \"\"\"\n",
    "    Extrae la fecha  en el nombre de la zona, e.g. 'botadero‑YYYY‑MM‑DD'.\n",
    "    Devuelve None si el patrón no coincide.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, m, d = zona_id.split('-')[-3:]\n",
    "        return dt.date(int(y), int(m), int(d))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def coverage_filter(geom, min_coverage, area_tot, cob, band):\n",
    "    def _fn(img):\n",
    "        return img\n",
    "    return _fn\n",
    "\n",
    "def add_inside_flag(puntos_fc, geom_z, zona_id):\n",
    "    return puntos_fc.map(\n",
    "        lambda f: f.set({\n",
    "            'inside': geom_z.contains(f.geometry(), maxError=1),\n",
    "            'zona_actual': zona_id\n",
    "        })\n",
    "    )\n",
    "\n",
    "def fc_to_dataframe(fc, batch_size=5000) -> pd.DataFrame:\n",
    "    try:\n",
    "        size = fc.size().getInfo()\n",
    "    except Exception:\n",
    "        size = batch_size\n",
    "    dfs = []\n",
    "    for start in range(0, size, batch_size):\n",
    "        sub_fc   = ee.FeatureCollection(fc.toList(batch_size, start))\n",
    "        sub_info = sub_fc.getInfo()\n",
    "        dfs.append(\n",
    "            pd.json_normalize([f['properties'] for f in sub_info['features']])\n",
    "        )\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "def multitemporal_bivar_changes_2(\n",
    "        distrubed_zones: ee.FeatureCollection,\n",
    "        puntos_fc: ee.FeatureCollection,\n",
    "        start_date: str = '2017-01-01',\n",
    "        end_date: str = '2025-12-31',\n",
    "        filepath: str = r\"C:\\Users\\Natascha\\Desktop\\Tesis\\pantallazos\",\n",
    "        cob: float = 0.95,\n",
    "        show_map: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "    out_csv = os.path.join(filepath, 'cambios_punto_a_punto2_15000.csv')\n",
    "\n",
    "    zonas_info = []\n",
    "    for f in distrubed_zones.getInfo()['features']:\n",
    "        zona_id = f['properties']['zona']\n",
    "        fecha = _fecha_de_nombre(zona_id)\n",
    "        if fecha:\n",
    "            zonas_info.append({\n",
    "                'zona_id': zona_id,\n",
    "                'fecha': fecha,\n",
    "                'geom': ee.Feature(f).geometry()\n",
    "            })\n",
    "    if not zonas_info:\n",
    "        print(\"[WARN] Ninguna zona tiene fecha embebida\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    zonas_info.sort(key=lambda z: z['fecha'])\n",
    "    fechas_ord = [z['fecha'] for z in zonas_info]\n",
    "    \n",
    "    zonas_union = distrubed_zones.geometry().bounds()\n",
    "    S1_global = (ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT')\n",
    "                 .filterBounds(zonas_union)\n",
    "                 .filterDate(start_date, end_date)\n",
    "                 .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n",
    "                 .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "                 .select(['VV', 'VH'])\n",
    "                 .sort('system:time_start'))\n",
    "    total_imgs = S1_global.size().getInfo()\n",
    "    print(f\"Imágenes Sentinel‑1 (DESCENDING, VH) 2017‑25: {total_imgs}\")\n",
    "\n",
    "    def _nearest_zona(fecha_img: dt.date) -> dict:\n",
    "        pos = bisect_left(fechas_ord, fecha_img)\n",
    "        cand = []\n",
    "        if pos:\n",
    "            cand.append(zonas_info[pos-1])\n",
    "        if pos < len(fechas_ord):\n",
    "            cand.append(zonas_info[pos])\n",
    "        return min(cand, key=lambda z: abs(z['fecha'] - fecha_img))\n",
    "\n",
    "    def _label_points(c_map, geom_z, zona_id, fecha_img):\n",
    "        malla_flag = add_inside_flag(puntos_fc, geom_z, zona_id)\n",
    "        sample = (c_map.unmask(-1).rename('code')\n",
    "                  .sampleRegions(malla_flag,\n",
    "                                 properties=['numero', 'inside', 'zona_actual'],\n",
    "                                 scale=10, tileScale=4))\n",
    "        df = fc_to_dataframe(sample).rename(columns={'numero': 'punto_id'})\n",
    "        df['fecha'] = pd.to_datetime(fecha_img)\n",
    "        df['cambio'] = df['code'].map({-1: 'fuera', 0: 'sin cambios',\n",
    "                                       1: 'indefinido', 2: 'alzamiento',\n",
    "                                       3: 'subsidencia'})\n",
    "        return df[['punto_id', 'zona_actual', 'inside', 'fecha', 'cambio']]\n",
    "\n",
    "    imgs_list = S1_global.toList(total_imgs)\n",
    "    fechas_ms = S1_global.aggregate_array('system:time_start').getInfo()\n",
    "    fechas_dt = [dt.datetime.utcfromtimestamp(ms/1000).date() for ms in fechas_ms]\n",
    "\n",
    "    header_csv = True\n",
    "    puntos_global = []\n",
    "\n",
    "    for k in range(total_imgs - 1):\n",
    "        im1 = ee.Image(imgs_list.get(k))\n",
    "        im2 = ee.Image(imgs_list.get(k+1))\n",
    "        fecha_inicio = fechas_dt[k]\n",
    "        fecha_fin = fechas_dt[k+1]\n",
    "\n",
    "        zona_info = _nearest_zona(fecha_fin)\n",
    "        zona_id = zona_info['zona_id']\n",
    "        geom_z = zona_info['geom']\n",
    "        area_tot = geom_z.area(maxError=1).getInfo()\n",
    "\n",
    "        pair_ic = ee.ImageCollection([im1, im2])\n",
    "        for band in ['VV']:\n",
    "            pair_ic = pair_ic.map(coverage_filter(geom_z, 100, area_tot, cob, band))\n",
    "        if pair_ic.size().getInfo() < 2:\n",
    "            print(f\"[SKIP] {fecha_fin} → pares sin cobertura suficiente\")\n",
    "            continue\n",
    "        im1 = ee.Image(pair_ic.toList(2).get(0))\n",
    "        im2 = ee.Image(pair_ic.toList(2).get(1))\n",
    "\n",
    "        #  cálculo de m2logQ, p_value, c_map \n",
    "        def det(im):\n",
    "            return im.expression('b(0) * b(1)')\n",
    "\n",
    "        list1 = []\n",
    "        list2 = []\n",
    "\n",
    "        for xx in range(0, 31, 1):\n",
    "            m = 2 + xx * 0.1\n",
    "            m2logQ = det(im1).log()\\\n",
    "                      .add(det(im2).log())\\\n",
    "                      .subtract(det(im1.add(im2)).log().multiply(2))\\\n",
    "                      .add(4 * np.log(2))\\\n",
    "                      .multiply(-2 * m)\n",
    "\n",
    "            hist = m2logQ.reduceRegion(\n",
    "                ee.Reducer.fixedHistogram(0, 20, 200),\n",
    "                geom_z\n",
    "            ).get('VV').getInfo()\n",
    "            aa = np.array(hist)\n",
    "            x = aa[:, 0]\n",
    "            y = aa[:, 1] / np.sum(aa[:, 1])\n",
    "            yyy = (np.abs(y - (chi2.pdf(x, 2) / 10)) /\n",
    "                   ((y + (chi2.pdf(x, 2) / 10)) / 2)).sum()\n",
    "\n",
    "            list1.append(m)\n",
    "            list2.append(yyy)\n",
    "\n",
    "        opt_array = np.array([list1, list2])\n",
    "        numb = np.where(opt_array[1, :] == opt_array[1, :].min())[0][0]\n",
    "\n",
    "        ### óptima\n",
    "        m = opt_array[0, numb]\n",
    "        m2logQ = det(im1).log()\\\n",
    "                  .add(det(im2).log())\\\n",
    "                  .subtract(det(im1.add(im2)).log().multiply(2))\\\n",
    "                  .add(4 * np.log(2))\\\n",
    "                  .multiply(-2 * m)\n",
    "\n",
    "        def chi2cdf(chi2_img, df):\n",
    "            ''' Chi square CDF using gammainc '''\n",
    "            return ee.Image(chi2_img.divide(2)).gammainc(ee.Number(df).divide(2))\n",
    "\n",
    "        p_value = ee.Image.constant(1).subtract(chi2cdf(m2logQ, 2))\n",
    "\n",
    "        hist = p_value.reduceRegion(ee.Reducer.fixedHistogram(0, 1, 100),geom_z).get('constant').getInfo()\n",
    "        aa = np.array(hist)\n",
    "        x = aa[:, 0]\n",
    "        y = aa[:, 1] / np.sum(aa[:, 1])\n",
    "\n",
    "        plt.plot(x, y, '.b', label='p-value')\n",
    "        plt.ylim(0, 0.05)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        c_map = p_value.multiply(0).where(p_value.lt(0.05), 1)\n",
    "        diff = im2.subtract(im1)\n",
    "        d_map = c_map.multiply(0)                    # 0 sin cambios\n",
    "        d_map = d_map.where(det(diff).gt(0), 2)      # 2 alzamientos\n",
    "        d_map = d_map.where(diff.select(0).gt(0), 3) # 3 subsidencias\n",
    "        d_map = d_map.where(det(diff).lt(0), 1)      # 1 indefinido\n",
    "        c_map = c_map.multiply(d_map)\n",
    "\n",
    "        if show_map:\n",
    "            import geemap, webbrowser, pathlib\n",
    "\n",
    "            Map = geemap.Map(height=600)\n",
    "            Map.centerObject(zona_info['geom'], 13)\n",
    "\n",
    "            changed_clip = c_map.select('constant').clip(zona_info['geom'])\n",
    "\n",
    "            vis = {\n",
    "                'min': 0, 'max': 3,\n",
    "                'palette': ['black', 'red', 'cyan', 'yellow']\n",
    "            }\n",
    "            Map.addLayer(\n",
    "                changed_clip,\n",
    "                vis,\n",
    "                f'Cambios {zona_id} '\n",
    "                f'{fecha_inicio.strftime(\"%Y-%m-%d\")} → '\n",
    "                f'{fecha_fin.strftime(\"%Y-%m-%d\")}',\n",
    "                shown=True\n",
    "            )\n",
    "\n",
    "            Map.addLayer(zona_info['geom'], {'color': 'white'}, 'Límite zona_actual', True)\n",
    "\n",
    "            nombre_html = os.path.join(\n",
    "                filepath,\n",
    "                f'cambios_{zona_id}_'\n",
    "                f'{fecha_inicio.strftime(\"%Y%m%d\")}_'\n",
    "                f'{fecha_fin.strftime(\"%Y%m%d\")}.html'\n",
    "            )\n",
    "            Map.to_html(nombre_html)\n",
    "            print(f\"[INFO] Mapa guardado en {nombre_html}\")\n",
    "            try:\n",
    "                os.startfile(nombre_html)\n",
    "            except AttributeError:\n",
    "                webbrowser.open(pathlib.Path(nombre_html).resolve().as_uri())\n",
    "                \n",
    "        df_pts = _label_points(c_map, geom_z, zona_id, fecha_fin)\n",
    "        if not df_pts.empty:\n",
    "            df_pts.to_csv(out_csv, mode='a', header=header_csv, index=False)\n",
    "            header_csv = False\n",
    "            puntos_global.append(df_pts)\n",
    "\n",
    "\n",
    "    if puntos_global:\n",
    "        return pd.concat(puntos_global, ignore_index=True)\n",
    "    print(\"[INFO] No se generaron puntos.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "dire=r\"C:\\Users\\carpeta\"\n",
    "\n",
    "\n",
    "malla_de_puntos =  ee.FeatureCollection(\"projects/ee-projects/assets/15000_numerados\")\n",
    "Feature1 = ee.FeatureCollection(\"projects/ee-projects/assets/2023\")\n",
    "a=Feature1.first()\n",
    "\n",
    "col_names=pd.DataFrame.from_dict(a.toDictionary(a.propertyNames()).getInfo(), orient='index').reset_index()\n",
    "col_names=list(col_names['index'])\n",
    "\n",
    "Feature1.aggregate_count_distinct('Name').getInfo()\n",
    "\n",
    "\n",
    "for Muestra in list(pd.Series(Feature1.aggregate_array('Name').getInfo()).unique()):\n",
    "    \n",
    "    mined_area = Feature1.filter(ee.Filter.stringContains('Name',\"area_mina\"))\n",
    "    mined_area.aggregate_array('COD_SSUBC').getInfo()\n",
    "    vector = mined_area.geometry().bounds()\n",
    "    filepath=dire + '\\\\' + Muestra + '\\\\'\n",
    "    try:\n",
    "      os.makedirs(dire + '\\\\' + Muestra )\n",
    "    except:\n",
    "      print (\"folder already created\")\n",
    "    final_date='2025-06-13'\n",
    "    ALPHA = 0.05        \n",
    "    error=5\n",
    "    cob = 0.95\n",
    "    buffer=250\n",
    "    significancia=0.95\n",
    "    distrubed_zones = ee.FeatureCollection(\"projects/ee-projects/assets/botadero2017-2025\")\n",
    "    \n",
    "\n",
    "def main() -> None:\n",
    "\n",
    "    try:\n",
    "        ee.Initialize(project='ee-projects')\n",
    "    except ee.EEException:\n",
    "        ee.Authenticate()\n",
    "        ee.Initialize(project='ee-projects')\n",
    "\n",
    "    START = '2017-01-01'\n",
    "    END   = '2025-12-31'\n",
    "\n",
    "    OUT_DIR = Path(r\"C:\\Users\\Natascha\\Desktop\\Tesis\\pantallazos\")\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ZONAS_FC = ee.FeatureCollection(\n",
    "        \"projects/ee-projects/assets/botadero2017-2025\"\n",
    "    )\n",
    "    PUNTOS_FC = ee.FeatureCollection(\n",
    "        \"projects/ee-projects/assets/15000_numerados\"\n",
    "    )\n",
    "\n",
    "    df_result = multitemporal_bivar_changes_2(\n",
    "        distrubed_zones = ZONAS_FC,\n",
    "        puntos_fc       = PUNTOS_FC,\n",
    "        start_date      = START,\n",
    "        end_date        = END,\n",
    "        filepath        = str(OUT_DIR),\n",
    "        cob             = 0.95,\n",
    "        show_map        = True   \n",
    "    )\n",
    "\n",
    "    if not df_result.empty:\n",
    "        csv_path = OUT_DIR / \"resumen_puntos_global_0.05.csv\"\n",
    "        df_result.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "        print(f\"\\n✓ DataFrame global guardado en: {csv_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d31bf0-b546-467b-bbd9-d1065b23ba91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007810b-2b1f-46df-b06e-b71ff6b93773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer la informacion de cada banda de cada satelite (L8,L9, S2) en cada punto_id \n",
    "\n",
    "import ee\n",
    "import datetime as dt\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "ZONAS_ASSET = \"projects/ee-projects/assets/botadero2017-2025\"\n",
    "MALLA_ASSET = \"projects/ee-projects/assets/nueva_malla_num_xy\"\n",
    "\n",
    "OUT_DIR = Path(r\"C:\\Users\\carpeta\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "FECHA_INI = \"2016-01-01\"\n",
    "FECHA_FIN = \"2025-12-31\"\n",
    "NUBE_MAX = 40\n",
    "\n",
    "PIXEL_COUNT = 15_000\n",
    "MIN_COVERAGE_FRAC = 0.90\n",
    "MIN_VALID_PIXELS = int(PIXEL_COUNT * MIN_COVERAGE_FRAC)  \n",
    "\n",
    "BATCH_SIZE_DESCARGA = 10590 \n",
    "TILE_SCALE = 4            \n",
    "SCALE_VALID = 30         \n",
    "\n",
    "DO_L7 = True\n",
    "DO_L8 = True\n",
    "DO_L9 = True\n",
    "DO_S2 = True\n",
    "\n",
    "BANDAS_L7_SR = [\"SR_B1\", \"SR_B2\", \"SR_B3\", \"SR_B4\", \"SR_B5\", \"SR_B7\"]\n",
    "BANDAS_L7_TH = [\"ST_B6\"]\n",
    "BANDAS_L7 = BANDAS_L7_SR + BANDAS_L7_TH\n",
    "\n",
    "BANDAS_L8_SR = [\"SR_B1\", \"SR_B2\", \"SR_B3\", \"SR_B4\", \"SR_B5\", \"SR_B6\", \"SR_B7\"]\n",
    "BANDAS_L8_TH = [\"ST_B10\"]\n",
    "BANDAS_L8 = BANDAS_L8_SR + BANDAS_L8_TH\n",
    "\n",
    "BANDAS_S2 = [\n",
    "    \"B1\",\n",
    "    \"B2\",\n",
    "    \"B3\",\n",
    "    \"B4\",\n",
    "    \"B5\",\n",
    "    \"B6\",\n",
    "    \"B7\",\n",
    "    \"B8\",\n",
    "    \"B8A\",\n",
    "    \"B9\",\n",
    "    \"B11\",\n",
    "    \"B12\",\n",
    "]\n",
    "\n",
    "def extraer_fecha(nombre: str) -> Optional[dt.date]:\n",
    "    \"\"\"Extrae fecha YYYY‑MM‑DD o DD‑MM‑YYYY de un string.\"\"\"\n",
    "    m = re.search(r\"(\\d{4})[-_](\\d{2})[-_](\\d{2})\", nombre)\n",
    "    if m:\n",
    "        y, mo, d = map(int, m.groups())\n",
    "        return dt.date(y, mo, d)\n",
    "    m = re.search(r\"(\\d{2})[-_](\\d{2})[-_](\\d{4})\", nombre)\n",
    "    if m:\n",
    "        d, mo, y = map(int, m.groups())\n",
    "        return dt.date(y, mo, d)\n",
    "    return None\n",
    "\n",
    "def preparar_zonas(zonas_fc: ee.FeatureCollection) -> List[Dict]:\n",
    "    \"\"\"Descarga una vez las zonas y retorna lista con nombre, fecha y geom.\"\"\"\n",
    "    zonas_list = zonas_fc.getInfo()[\"features\"]\n",
    "    out: List[Dict] = []\n",
    "    for z in zonas_list:\n",
    "        nombre = z[\"properties\"].get(\"zona\") or z[\"properties\"].get(\"name\", \"\")\n",
    "        fz = extraer_fecha(nombre)\n",
    "        if fz:\n",
    "            out.append({\n",
    "                \"nombre\": nombre,\n",
    "                \"fecha\": fz,\n",
    "                \"geom\": ee.Geometry(z[\"geometry\"]),\n",
    "            })\n",
    "    out.sort(key=lambda d: d[\"fecha\"])\n",
    "    if not out:\n",
    "        raise ValueError(\"No se pudieron extraer fechas válidas de las zonas.\")\n",
    "    return out\n",
    "\n",
    "def escalar_l7(img: ee.Image) -> ee.Image:\n",
    "    sr = img.select(BANDAS_L7_SR).multiply(0.0000275).add(-0.2).clamp(0, 1)\n",
    "    tc = (img.select(\"ST_B6\")\n",
    "            .multiply(0.00341802)\n",
    "            .add(149)\n",
    "            .subtract(273.15)\n",
    "            .rename(\"Temp_C\"))\n",
    "    scaled = sr.addBands(tc).copyProperties(img, img.propertyNames())\n",
    "    return ee.Image(scaled)           \n",
    "\n",
    "def escalar_l8l9(img: ee.Image) -> ee.Image:\n",
    "    sr = img.select(BANDAS_L8_SR).multiply(0.0000275).add(-0.2).clamp(0, 1)\n",
    "    tc = (img.select(\"ST_B10\")\n",
    "            .multiply(0.00341802)\n",
    "            .add(149)\n",
    "            .subtract(273.15)\n",
    "            .rename(\"Temp_C\"))\n",
    "    scaled = sr.addBands(tc).copyProperties(img, img.propertyNames())\n",
    "    return ee.Image(scaled)\n",
    "\n",
    "def escalar_s2(img: ee.Image) -> ee.Image:\n",
    "    scaled = img.select(BANDAS_S2).divide(10000) \\\n",
    "                .copyProperties(img, img.propertyNames())\n",
    "    return ee.Image(scaled)\n",
    "\n",
    "def add_valid_and_date(img: ee.Image, band_ref: str, geom: ee.Geometry) -> ee.Image:\n",
    "    mask = img.select([band_ref]).mask().gt(0)\n",
    "    nval = mask.reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        geometry=geom,\n",
    "        scale=SCALE_VALID,  \n",
    "        maxPixels=1e9,\n",
    "    ).get(band_ref)\n",
    "    return img.set({\"n_valid\": nval, \"date_only\": img.date().format(\"YYYY-MM-dd\")})\n",
    "\n",
    "\n",
    "def preparar_coleccion(\n",
    "    collection_id: str,\n",
    "    bandas: List[str],\n",
    "    nube_prop: str,\n",
    "    geom_bbox: ee.Geometry,\n",
    ") -> ee.ImageCollection:\n",
    "    band_ref = bandas[0]\n",
    "    return (\n",
    "        ee.ImageCollection(collection_id)\n",
    "        .filterDate(FECHA_INI, FECHA_FIN)\n",
    "        .filterBounds(geom_bbox)\n",
    "        .filter(ee.Filter.lt(nube_prop, NUBE_MAX))\n",
    "        .select(bandas)\n",
    "        .map(lambda im: add_valid_and_date(im, band_ref, geom_bbox))\n",
    "        .filter(ee.Filter.gte(\"n_valid\", MIN_VALID_PIXELS))\n",
    "        .sort(\"n_valid\", False)\n",
    "        .distinct(\"date_only\")\n",
    "    )\n",
    "\n",
    "def descargar_fc_csv(fc: ee.FeatureCollection, columnas: List[str]) -> pd.DataFrame:\n",
    "    url = fc.getDownloadURL(\n",
    "        filetype=\"csv\",         \n",
    "        selectors=columnas,    \n",
    "        filename=\"tmp\"          \n",
    "    )\n",
    "    return pd.read_csv(url)\n",
    "\n",
    "def muestrear_imagenes(\n",
    "    col: ee.ImageCollection,\n",
    "    escalar_fn,\n",
    "    nube_prop: str,\n",
    "    bandas_finales: List[str],\n",
    "    mallas_zona: Dict[str, ee.FeatureCollection],\n",
    "    zonas_info: List[Dict],\n",
    "    sensor_tag: str,\n",
    ") -> pd.DataFrame:\n",
    "    ids = col.aggregate_array(\"system:index\").getInfo()\n",
    "    print(f\"\\n[{sensor_tag}] Imágenes filtradas: {len(ids)}\")\n",
    "\n",
    "    filas_acumuladas: List[pd.DataFrame] = []\n",
    "    t_global = time.time()\n",
    "\n",
    "    for i, idx in enumerate(ids, 1):\n",
    "        print(f\"    → ({i}/{len(ids)}) comenzando {sensor_tag}  {idx}\")\n",
    "        t0 = time.time()\n",
    "        im = ee.Image(col.filter(ee.Filter.eq(\"system:index\", idx)).first())\n",
    "        fecha_ms = im.date().millis().getInfo()\n",
    "        fecha_img = dt.datetime.utcfromtimestamp(fecha_ms / 1_000).date()\n",
    "        zsel = min(zonas_info, key=lambda z: abs((z[\"fecha\"] - fecha_img).days))\n",
    "        malla_etq = mallas_zona[zsel[\"nombre\"]]\n",
    "        img_sc = ee.Image(escalar_fn(im)) \n",
    "        samp = (\n",
    "            img_sc.sampleRegions(\n",
    "                collection=malla_etq,\n",
    "                properties=[\"numero\", \"zona_actual\", \"inside\"],\n",
    "                scale=30,\n",
    "                tileScale=TILE_SCALE,\n",
    "            )\n",
    "            .map(\n",
    "                lambda f: f.set(\n",
    "                    {\n",
    "                        \"fecha_img\": im.date().format(\"YYYY-MM-dd\"),\n",
    "                        \"cloud\": im.get(nube_prop),\n",
    "                        \"sensor\": sensor_tag,\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        columnas = (\n",
    "            [\"numero\", \"zona_actual\", \"inside\", \"fecha_img\", \"cloud\", \"sensor\"]\n",
    "            + bandas_finales\n",
    "            + [\"Temp_C\"]\n",
    "        )\n",
    "        df = descargar_fc_csv(samp, columnas).rename(columns={\"numero\": \"punto_id\"})\n",
    "\n",
    "        filas_acumuladas.append(df)\n",
    "\n",
    "        print(\n",
    "            f\"       filas: {len(df):,} | fecha {fecha_img} | zona {zsel['nombre']} | {time.time() - t0:0.1f}s\"\n",
    "        )\n",
    "\n",
    "    df_final = pd.concat(filas_acumuladas, ignore_index=True) if filas_acumuladas else pd.DataFrame()\n",
    "    print(f\"[{sensor_tag}] TOTAL filas: {len(df_final):,} | Tiempo total: {time.time() - t_global:0.1f}s\")\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def main():\n",
    "    zonas_fc = ee.FeatureCollection(ZONAS_ASSET)\n",
    "    malla_fc = ee.FeatureCollection(MALLA_ASSET)\n",
    "    \n",
    "    zonas_info = preparar_zonas(zonas_fc)\n",
    "    print(\n",
    "        f\"Zonas válidas: {len(zonas_info)} (ej: {zonas_info[0]['nombre']} → {zonas_info[0]['fecha']})\"\n",
    "    )\n",
    "\n",
    "    bbox = zonas_fc.geometry()\n",
    "\n",
    "    mallas_zona = {\n",
    "        z[\"nombre\"]: malla_fc.map(\n",
    "            lambda pt, g=z[\"geom\"], n=z[\"nombre\"]: pt.set(\n",
    "                {\"inside\": g.contains(pt.geometry()), \"zona_actual\": n}\n",
    "            )\n",
    "        )\n",
    "        for z in zonas_info\n",
    "    }\n",
    "\n",
    "    resultados: List[Tuple[str, pd.DataFrame]] = []\n",
    "\n",
    "\n",
    "    #if DO_L7:\n",
    "     #   col = preparar_coleccion(\"LANDSAT/LE07/C02/T1_L2\", BANDAS_L7, \"CLOUD_COVER\", bbox)\n",
    "    #    df = muestrear_imagenes(\n",
    "    #        col, escalar_l7, \"CLOUD_COVER\", BANDAS_L7_SR, mallas_zona, zonas_info, \"L7\"\n",
    "      #  )\n",
    "    #    resultados.append((\"valores_L7_puntos2_15000.csv\", df))\n",
    "\n",
    "    if DO_L8:\n",
    "        col = preparar_coleccion(\"LANDSAT/LC08/C02/T1_L2\", BANDAS_L8, \"CLOUD_COVER\", bbox)\n",
    "        df = muestrear_imagenes(\n",
    "            col, escalar_l8l9, \"CLOUD_COVER\", BANDAS_L8_SR, mallas_zona, zonas_info, \"L8\"\n",
    "        )\n",
    "        resultados.append((\"valores_L8_puntos_15000.csv\", df))\n",
    "\n",
    "    if DO_L9:\n",
    "        col = preparar_coleccion(\"LANDSAT/LC09/C02/T1_L2\", BANDAS_L8, \"CLOUD_COVER\", bbox)\n",
    "        df = muestrear_imagenes(\n",
    "            col, escalar_l8l9, \"CLOUD_COVER\", BANDAS_L8_SR, mallas_zona, zonas_info, \"L9\"\n",
    "        )\n",
    "        resultados.append((\"valores_L9_puntos_15000.csv\", df))\n",
    "\n",
    "    if DO_S2:\n",
    "        col = preparar_coleccion(\n",
    "            \"COPERNICUS/S2_SR_HARMONIZED\",\n",
    "            BANDAS_S2,\n",
    "            \"CLOUDY_PIXEL_PERCENTAGE\",\n",
    "            bbox,\n",
    "        )\n",
    "        df = muestrear_imagenes(\n",
    "            col, escalar_s2, \"CLOUDY_PIXEL_PERCENTAGE\", BANDAS_S2, mallas_zona, zonas_info, \"S2\"\n",
    "        )\n",
    "        resultados.append((\"valores_S2_puntos_15000.csv\", df))\n",
    "\n",
    "    for nombre, df in resultados:\n",
    "        out_path = OUT_DIR / nombre\n",
    "        df.to_csv(out_path, sep=\";\", decimal=\".\", index=False)\n",
    "        print(f\" Guardado: {out_path} ({len(df)} filas)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc1a584-dfb6-4160-9bc9-3c406999a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEM\n",
    "\n",
    "import ee, pandas as pd\n",
    "from pathlib import Path\n",
    "ee.Initialize()\n",
    "\n",
    "dem = ee.Image(\"NASA/NASADEM_HGT/001\")\n",
    "malla = ee.FeatureCollection(\"projects/ee-nataschavonsengerloeper123/assets/nueva_malla_num_xy\")\n",
    "\n",
    "sample = dem.sampleRegions(\n",
    "    collection = malla,\n",
    "    properties = ['numero','zona_actual'], \n",
    "    scale      = 30,   # NASADEM es a 30 m\n",
    "    tileScale  = 4\n",
    ")\n",
    "\n",
    "count      = sample.size().getInfo()\n",
    "batch_size = 5000\n",
    "props = []\n",
    "for offset in range(0, count, batch_size):\n",
    "    batch = sample.toList(batch_size, offset).getInfo()\n",
    "    props.extend(feat['properties'] for feat in batch)\n",
    "\n",
    "df = pd.json_normalize(props)\n",
    "out_csv = Path(r\"C:\\Users\\malla_elevacion_NASADEM.csv\")\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f\"Elevaciones guardadas en {out_csv} ({len(df):,} puntos)\")\n",
    "\n",
    "#  DEM GLO-30 \n",
    "dem = ee.ImageCollection(\"COPERNICUS/DEM/GLO30\").mosaic()  \n",
    "malla = ee.FeatureCollection(\"projects/ee-projects/assets/nueva_malla_num_xy\")\n",
    "\n",
    "sample = dem.sampleRegions(\n",
    "    collection = malla,\n",
    "    properties = ['numero','zona_actual'],  \n",
    "    scale      = 30,\n",
    "    tileScale  = 4\n",
    ")\n",
    "\n",
    "count      = sample.size().getInfo()\n",
    "batch_size = 5000\n",
    "\n",
    "props = []\n",
    "for offset in range(0, count, batch_size):\n",
    "    batch = sample.toList(batch_size, offset).getInfo()\n",
    "    props.extend(f['properties'] for f in batch)\n",
    "\n",
    "# 4) DataFrame y CSV\n",
    "df = pd.json_normalize(props)\n",
    "out_csv = Path(r\"C:\\Users\\\\malla_elev_GLO30.csv\")\n",
    "df.to_csv(\n",
    "    out_csv,\n",
    "    sep=';',           \n",
    "    decimal='.',       \n",
    "    float_format='%.3f',  \n",
    "    index=False\n",
    ")\n",
    "print(f\"CSV exportado en: {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b8d93-0a2d-472c-be75-f69143c1d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos Clima\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import ee\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def weather_eras(dire: str,\n",
    "                 Feature1: ee.FeatureCollection,\n",
    "                 Muestra: str,\n",
    "                 initial_date: str,\n",
    "                 final_date: str) -> pd.DataFrame:\n",
    "    print(\"→ Starting weather_eras\")\n",
    "    filepath = os.path.join(dire, Muestra, 'weather_eras')\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "    print(f\"   * Output folder: {filepath}\")\n",
    "    filt = ee.Filter.inList('zona', [Muestra])\n",
    "    geom = Feature1.filter(filt).first().geometry()\n",
    "    col = (ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR')\n",
    "           .filterDate(initial_date, final_date)\n",
    "           .filterBounds(geom))\n",
    "    ids = col.aggregate_array('system:index').getInfo()\n",
    "    print(f\"   * {len(ids)} daily scenes between {initial_date} and {final_date}\")\n",
    "\n",
    "    # Bandas diarias\n",
    "    daily_bands = [\n",
    "        'dewpoint_temperature_2m','temperature_2m','skin_temperature',\n",
    "        'soil_temperature_level_1','soil_temperature_level_2',\n",
    "        'soil_temperature_level_3','soil_temperature_level_4',\n",
    "        'lake_bottom_temperature','lake_ice_depth','lake_ice_temperature',\n",
    "        'lake_mix_layer_depth','lake_mix_layer_temperature','lake_shape_factor',\n",
    "        'lake_total_layer_temperature','snow_albedo','snow_cover','snow_density',\n",
    "        'snow_depth','snow_depth_water_equivalent','snowfall_sum','snowmelt_sum',\n",
    "        'temperature_of_snow_layer','skin_reservoir_content',\n",
    "        'volumetric_soil_water_layer_1','volumetric_soil_water_layer_2',\n",
    "        'volumetric_soil_water_layer_3','volumetric_soil_water_layer_4',\n",
    "        'forecast_albedo','surface_latent_heat_flux',\n",
    "        'surface_net_solar_radiation','surface_net_thermal_radiation',\n",
    "        'surface_sensible_heat_flux','surface_solar_radiation_downwards',\n",
    "        'surface_thermal_radiation_downwards','evaporation_from_bare_soil',\n",
    "        'evaporation_from_open_water_surfaces_excluding_oceans',\n",
    "        'evaporation_from_the_top_of_canopy',\n",
    "        'evaporation_from_vegetation_transpiration',\n",
    "        'potential_evaporation_sum','runoff_sum','snow_evaporation_sum',\n",
    "        'sub_surface_runoff_sum','surface_runoff_sum','total_evaporation_sum',\n",
    "        'u_component_of_wind_10m','v_component_of_wind_10m','surface_pressure',\n",
    "        'total_precipitation_sum','leaf_area_index_high_vegetation',\n",
    "        'leaf_area_index_low_vegetation'\n",
    "    ]\n",
    "    \n",
    "    records: list[dict] = []\n",
    "\n",
    "    for idx in ids:\n",
    "        print(f\"Processing {idx}\")\n",
    "        img = ee.Image(f'ECMWF/ERA5_LAND/DAILY_AGGR/{idx}').clip(geom)\n",
    "        red = img.reduceRegion(\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            geometry=geom,\n",
    "            scale=100,\n",
    "            bestEffort=True\n",
    "        ).getInfo()\n",
    "        red.update({\n",
    "            'year':  int(idx[:4]),\n",
    "            'month': int(idx[4:6]),\n",
    "            'day':   int(idx[6:8]),\n",
    "            'stat':  'mean'\n",
    "        })\n",
    "        records.append(red)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=daily_bands + ['year','month','day','stat'])\n",
    "    \n",
    "    out_path = Path(filepath) / 'weather_daily_tot.xlsx'\n",
    "    df.to_excel(out_path, index=False)\n",
    "    print(f\"   * Saved to {out_path}\")\n",
    "    return df\n",
    "\n",
    "def get_largest_feature_name(asset_path: str, name_prop: str = 'zona') -> Optional[str]:\n",
    "    print(\"→ Selecting feature with largest area...\")\n",
    "    fc = ee.FeatureCollection(asset_path)\n",
    "    with_areas = fc.map(lambda f: f.set('area_m2', f.geometry().area(1)))\n",
    "    largest = ee.Feature(with_areas.sort('area_m2', False).first())\n",
    "    name = largest.get(name_prop).getInfo()\n",
    "    print(f\"   * Selected: {name}\")\n",
    "    return name\n",
    "\n",
    "def main():\n",
    "    dire = r\"C:\\Users\\carpeta\"\n",
    "    asset = 'projects/ee-projects/assets/botadero2017-2025'\n",
    "    initial = '2016-01-01'\n",
    "    final   = '2025-12-31'\n",
    "    \n",
    "    os.makedirs(dire, exist_ok=True)\n",
    "    zona = get_largest_feature_name(asset, name_prop='zona')\n",
    "    print(f\" Running weather_eras for zone: {zona}\")\n",
    "    df_wea = weather_eras(dire, ee.FeatureCollection(asset), zona, initial, final)\n",
    "\n",
    "    print(\"Finished. Sample output:\")\n",
    "    print(df_wea.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89157689-7d12-4de9-a1a5-ce5019048159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d76281-92ed-475b-8238-2e5fdc5dd89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d971ab-7d11-4d73-be73-3b89cfc9428f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9aba3c-f4fa-49f2-a858-c6253af7fd66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82995418-b416-4bde-b146-14b7a41fefbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10c9a7-2a3d-404b-ac9b-19a474de9eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
