{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d392d4-3454-4da3-90b9-8d09a6762707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Junar los csv de datos de banda, de todos los sensores en 1 \n",
    "import os\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "folder = r\"C:\\Users\\Natascha\\Desktop\\Tesis\\pantallazos\\Nuevos_excel\"\n",
    "\n",
    "\n",
    "archivos = [\n",
    "    \"valores_S2_puntos_15000.csv\",\n",
    "    \"valores_L9_puntos_15000.csv\",\n",
    "    \"valores_L8_puntos_15000.csv\",\n",
    "]\n",
    "keys = [\"fecha_img\", \"zona_actual\", \"punto_id\"]\n",
    "\n",
    "def preparar_df(path, sensor):\n",
    "    df = pd.read_csv(path, sep=\";\", decimal=\".\", parse_dates=[\"fecha_img\"])\n",
    "    df[\"fecha_img\"]   = df[\"fecha_img\"].dt.normalize()          \n",
    "    df[\"zona_actual\"] = df[\"zona_actual\"].astype(str).str.strip()\n",
    "    df[\"punto_id\"]    = df[\"punto_id\"].astype(str).str.strip()\n",
    "    renames = {\n",
    "        col: f\"{col}_{sensor}\"\n",
    "        for col in df.columns\n",
    "        if col not in keys\n",
    "    }\n",
    "    return df.rename(columns=renames)\n",
    "\n",
    "dfs = []\n",
    "for fname in archivos:\n",
    "    path   = os.path.join(folder, fname)\n",
    "    sensor = fname.split(\"_\")[1]  \n",
    "    df     = preparar_df(path, sensor)\n",
    "    print(f\"{fname}: {len(df)} filas antes del merge\")\n",
    "    dfs.append(df)\n",
    "\n",
    "df_merged = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=keys, how=\"outer\"),\n",
    "    dfs\n",
    ")\n",
    "print(f\"\\nFilas totales después del merge: {len(df_merged)}\")\n",
    "\n",
    "\n",
    "for fname in archivos:\n",
    "    sensor = fname.split(\"_\")[1]\n",
    "    df_orig = pd.read_csv(\n",
    "        os.path.join(folder, fname),\n",
    "        sep=\";\", decimal=\".\", parse_dates=[\"fecha_img\"],\n",
    "        usecols=keys\n",
    "    )\n",
    "    # Normalizo \n",
    "    df_orig[\"fecha_img\"]   = df_orig[\"fecha_img\"].dt.normalize()\n",
    "    df_orig[\"zona_actual\"] = df_orig[\"zona_actual\"].astype(str).str.strip()\n",
    "    df_orig[\"punto_id\"]    = df_orig[\"punto_id\"].astype(str).str.strip()\n",
    "\n",
    "    check = pd.merge(\n",
    "        df_orig,\n",
    "        df_merged[keys],\n",
    "        on=keys,\n",
    "        how=\"left\",\n",
    "        indicator=True\n",
    "    )\n",
    "    lost = (check[\"_merge\"] == \"left_only\").sum()\n",
    "    total = len(df_orig)\n",
    "    status = \"OK\" if lost == 0 else f\"¡ERROR! {lost} perdidas\"\n",
    "    print(f\"  {fname}: {total} filas → {status}\")\n",
    "\n",
    "# Guardar \n",
    "out_path = os.path.join(folder, \"valores_bandas_10000.csv\")\n",
    "df_merged.to_csv(out_path, index=False, sep=\";\", decimal=\".\")\n",
    "print(\"Merge completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95742b2-2b92-4d19-b596-7f2fbc3b97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# junar datos de bandas con los datos de cambio de sentinel-1\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "folder    = r\"C:\\Users\\Natascha\\Desktop\\Tesis\\pantallazos\\Nuevos_excel\"\n",
    "file_val  = \"valores_bandas_10000.csv\"\n",
    "file_cam  = \"Cambio_nuevo_0.05.csv\"\n",
    "file_out  = \"valores_y_cambios_10000.csv\"\n",
    "\n",
    "path_val = os.path.join(folder, file_val)\n",
    "path_cam = os.path.join(folder, file_cam)\n",
    "\n",
    "\n",
    "df_val = pd.read_csv(path_val, sep=\";\", decimal=\".\", parse_dates=[\"fecha_img\"], low_memory=False)\n",
    "df_val[\"fecha_img\"]   = df_val[\"fecha_img\"].dt.normalize()\n",
    "df_val[\"zona_actual\"] = df_val[\"zona_actual\"].astype(str).str.strip()\n",
    "df_val[\"punto_id\"]    = df_val[\"punto_id\"].astype(str).str.strip()\n",
    "\n",
    "\n",
    "df_cam = pd.read_csv(path_cam, sep=\";\", decimal=\".\", low_memory=False)\n",
    "if len(df_cam.columns)==1 and \",\" in df_cam.columns[0]:\n",
    "    df_cam = pd.read_csv(path_cam, sep=\",\", decimal=\".\", low_memory=False)\n",
    "\n",
    "for cand in (\"fecha\",\"Fecha\",\"fecha_img\"):\n",
    "    if cand in df_cam.columns:\n",
    "        date_col = cand\n",
    "        break\n",
    "else:\n",
    "    raise KeyError(\"No encontré ninguna columna de fecha en cambios\")\n",
    "\n",
    "df_cam[date_col]      = pd.to_datetime(df_cam[date_col], infer_datetime_format=True, errors=\"raise\").dt.normalize()\n",
    "df_cam[\"zona_actual\"] = df_cam[\"zona_actual\"].astype(str).str.strip()\n",
    "df_cam[\"punto_id\"]    = df_cam[\"punto_id\"].astype(str).str.strip()\n",
    "\n",
    "\n",
    "df_merge = pd.merge(\n",
    "    df_val, df_cam,\n",
    "    left_on=[\"fecha_img\",\"zona_actual\",\"punto_id\"],\n",
    "    right_on=[date_col,  \"zona_actual\",\"punto_id\"],\n",
    "    how=\"outer\",\n",
    "    indicator=True,\n",
    "    suffixes=(\"\",\"_cambios\")\n",
    ")\n",
    "\n",
    "print(\"Valores sin match:\", (df_merge[\"_merge\"]==\"left_only\").sum())\n",
    "print(\"Cambios sin match:\", (df_merge[\"_merge\"]==\"right_only\").sum())\n",
    "print(\"Total filas resultantes:\", len(df_merge))\n",
    "\n",
    "\n",
    "df_merge[date_col] = df_merge[\"fecha_img\"].fillna(df_merge[date_col])\n",
    "if date_col != \"fecha\":\n",
    "    df_merge.rename(columns={date_col: \"fecha\"}, inplace=True)\n",
    "df_merge.drop(columns=[\"fecha_img\",\"_merge\"], inplace=True)\n",
    "\n",
    "\n",
    "out_path = os.path.join(folder, file_out)\n",
    "df_merge.to_csv(out_path, index=False, sep=\";\", decimal=\".\")\n",
    "print(\"Archivo guardado en:\", out_path)\n",
    "\n",
    "# Verificación \n",
    "keys_val = df_val[[\"fecha_img\",\"zona_actual\",\"punto_id\"]].drop_duplicates()\n",
    "keys_cam = df_cam[[date_col,\"zona_actual\",\"punto_id\"]].drop_duplicates()\n",
    "\n",
    "# renombrar column fecha_img → fecha para coincidencia\n",
    "keys_val = keys_val.rename(columns={\"fecha_img\":\"fecha\"})\n",
    "keys_cam = keys_cam.rename(columns={date_col:\"fecha\"})\n",
    "\n",
    "tuplas_val = list(keys_val.itertuples(index=False, name=None))\n",
    "tuplas_cam = list(keys_cam.itertuples(index=False, name=None))\n",
    "\n",
    "# tomar muestras aleatorias\n",
    "sample_val = random.sample(tuplas_val, min(10, len(tuplas_val)))\n",
    "sample_cam = random.sample(tuplas_cam, min(10, len(tuplas_cam)))\n",
    "\n",
    "print(\"\\nComprobando 10 filas aleatorias de VALORES:\")\n",
    "for fecha, zona, pid in sample_val:\n",
    "    exists = not df_merge[\n",
    "        (df_merge[\"fecha\"]==fecha) &\n",
    "        (df_merge[\"zona_actual\"]==zona) &\n",
    "        (df_merge[\"punto_id\"]==pid)\n",
    "    ].empty\n",
    "    print(f\"  {'OK' if exists else 'MISSING'} → {fecha}, {zona}, {pid}\")\n",
    "\n",
    "print(\"\\nComprobando 10 filas aleatorias de CAMBIOS:\")\n",
    "for fecha, zona, pid in sample_cam:\n",
    "    exists = not df_merge[\n",
    "        (df_merge[\"fecha\"]==fecha) &\n",
    "        (df_merge[\"zona_actual\"]==zona) &\n",
    "        (df_merge[\"punto_id\"]==pid)\n",
    "    ].empty\n",
    "    print(f\"  {'OK' if exists else 'MISSING'} → {fecha}, {zona}, {pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29935c-3a21-49bd-8be6-d7ec5a39ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fecha en una sola columna en el csv de clima\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Lee el archivo\n",
    "df = pd.read_excel(r\"C:\\Users\\Natascha\\Desktop\\Tesis\\pantallazos\\weather_daily_tot.xlsx\")\n",
    "\n",
    "df[\"fecha\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]]).dt.date\n",
    "\n",
    "df = df.drop(columns=[\"year\", \"month\", \"day\"])\n",
    "\n",
    "df.to_excel(\n",
    "    r\"C:\\Users\\Natascha\\Desktop\\Tesis\\pantallazos\\weather_daily_tot_con_fecha.xlsx\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b40bf-529f-4293-9359-efc7ef561486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Juntar clima con bandas y sentinel 1\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "folder         = r\"C:\\Users\\Natascha\\Desktop\\Tesis\\pantallazos\\Nuevos_excel\"\n",
    "file_weather   = \"weather_daily_tot_con_fecha.xlsx\"\n",
    "file_valores   = \"valores_y_cambios_10000.csv\"\n",
    "file_out       = \"banda_cambios_weather_10000.csv\"\n",
    "\n",
    "\n",
    "df_weather = pd.read_excel(os.path.join(folder, file_weather))\n",
    "df_weather[\"fecha\"] = pd.to_datetime(df_weather[\"fecha\"], errors=\"coerce\").dt.normalize()\n",
    "\n",
    "df_val = pd.read_csv(\n",
    "    os.path.join(folder, file_valores),\n",
    "    sep=\";\", decimal=\".\",\n",
    "    parse_dates=[\"fecha\"], low_memory=False\n",
    ")\n",
    "df_val[\"fecha\"]       = df_val[\"fecha\"].dt.normalize()\n",
    "df_val[\"zona_actual\"] = df_val[\"zona_actual\"].astype(str).str.strip()\n",
    "df_val[\"punto_id\"]    = df_val[\"punto_id\"].astype(str).str.strip()\n",
    "\n",
    "df_final = pd.merge(\n",
    "    df_val,\n",
    "    df_weather,\n",
    "    on=\"fecha\",\n",
    "    how=\"outer\",\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Filas originales de valores+cambios:\", len(df_val))\n",
    "print(\"Filas originales de weather:       \", len(df_weather))\n",
    "print(\"Filas tras merge:                  \", len(df_final))\n",
    "print(\"  – Sólo en valores+cambios:\", (df_final[\"_merge\"]==\"left_only\").sum())\n",
    "print(\"  – Sólo en weather:       \", (df_final[\"_merge\"]==\"right_only\").sum())\n",
    "print(\"  – En ambos:              \", (df_final[\"_merge\"]==\"both\").sum())\n",
    "\n",
    "#  Comprobación aleatoria \n",
    "keys_val = list(df_val[\"fecha\"].drop_duplicates())\n",
    "keys_wea = list(df_weather[\"fecha\"].drop_duplicates())\n",
    "\n",
    "sample_val = random.sample(keys_val, min(5, len(keys_val)))\n",
    "sample_wea = random.sample(keys_wea, min(5, len(keys_wea)))\n",
    "\n",
    "print(\"\\nChequeo aleatorio fechas de valores+cambios están en final:\")\n",
    "for d in sample_val:\n",
    "    exists = not df_final[df_final[\"fecha\"]==d].empty\n",
    "    print(f\"  {d.date()}: {'OK' if exists else 'MISSING'}\")\n",
    "\n",
    "print(\"\\nChequeo aleatorio fechas de weather están en final:\")\n",
    "for d in sample_wea:\n",
    "    exists = not df_final[df_final[\"fecha\"]==d].empty\n",
    "    print(f\"  {d.date()}: {'OK' if exists else 'MISSING'}\")\n",
    "\n",
    "#  Guardar \n",
    "df_final.drop(columns=[\"_merge\"], inplace=True)\n",
    "df_final.to_csv(os.path.join(folder, file_out), index=False, sep=\";\", decimal=\".\")\n",
    "print(f\"\\nArchivo guardado en: {file_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df1e82-5b39-40a3-a1cb-c17a963c1438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# junar el csv de coordendas de cada punto_id y la elevacion de cada punto segun el NASADEM y GLO3\n",
    "import os\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "folder = r\"C:\\Users\\Natascha\\Desktop\\Tesis\\pantallazos\\Nuevos_excel\"\n",
    "\n",
    "df_xy      = pd.read_csv(os.path.join(folder, \"X_Y_10000.csv\"), sep=\",\", low_memory=False)\n",
    "df_glo30   = pd.read_csv(os.path.join(folder, \"malla_elev_GLO30.csv\"), sep=\",\", low_memory=False)\n",
    "df_nasadem = pd.read_csv(os.path.join(folder, \"malla_elevacion_NASADEM.csv\"), sep=\",\", low_memory=False)\n",
    "\n",
    "\n",
    "for df in (df_xy, df_glo30, df_nasadem):\n",
    "    df[\"punto_id\"] = df[\"punto_id\"].astype(str).str.strip()\n",
    "\n",
    "df_merged = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=\"punto_id\", how=\"outer\"),\n",
    "    [df_xy, df_nasadem, df_glo30]\n",
    ")\n",
    "\n",
    "orig_count = len(df_xy)\n",
    "merged_count = len(df_merged)\n",
    "print(f\"\\nFilas en malla_antamina_XY: {orig_count}\")\n",
    "print(f\"Filas tras merge completo: {merged_count}\")\n",
    "assert merged_count >= orig_count, \"¡Se perdieron filas!\"\n",
    "\n",
    "out_csv = os.path.join(folder, \"malla_completa_antamina.csv\")\n",
    "df_merged.to_csv(out_csv, index=False, sep=\";\", decimal=\".\")\n",
    "print(f\"Merge final guardado en:\\n  {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1027dd-1e55-49d5-8506-d62026e3e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# junta el merge de coordenadas y dem con el csv de clima, bandas y cambios S1\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder         = r\"C:\\Users\\Natascha\\Desktop\\Tesis\\pantallazos\\Nuevos_excel\"\n",
    "file_val       = \"banda_cambios_weather_10000.csv\"\n",
    "file_malla     = \"malla_X_Y_elev.csv\"\n",
    "file_out       = \"clima_bandas_coordenadas_dem_10.csv\"\n",
    "\n",
    "path_val       = os.path.join(folder, file_val)\n",
    "path_malla     = os.path.join(folder, file_malla)\n",
    "path_out       = os.path.join(folder, file_out)\n",
    "\n",
    "\n",
    "df_malla = pd.read_csv(\n",
    "    path_malla,\n",
    "    sep=\";\",              \n",
    "    decimal=\".\",\n",
    "    low_memory=False,\n",
    "    dtype={\"punto_id\": \"string\"} \n",
    ")\n",
    "\n",
    "df_malla.columns = [c.strip().lstrip('\\ufeff') for c in df_malla.columns]\n",
    "\n",
    "df_malla[\"punto_id\"] = (\n",
    "    df_malla[\"punto_id\"]\n",
    "      .astype(str)\n",
    "      .str.strip()\n",
    "      .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Puntos en la malla original: {len(df_malla):,}\")\n",
    "\n",
    "chunksize   = 200_000\n",
    "first_write = True\n",
    "total_rows  = 0\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    path_val,\n",
    "    sep=\";\",              \n",
    "    decimal=\".\",\n",
    "    parse_dates=[\"fecha\"],\n",
    "    low_memory=False,\n",
    "    chunksize=chunksize\n",
    "):\n",
    "    \n",
    "    chunk.columns = [c.strip().lstrip('\\ufeff') for c in chunk.columns]\n",
    "\n",
    "    chunk[\"punto_id\"] = (\n",
    "        chunk[\"punto_id\"]\n",
    "          .astype(str)\n",
    "          .str.strip()\n",
    "          .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    merged = chunk.merge(df_malla, on=\"punto_id\", how=\"left\")\n",
    "    total_rows += len(merged)\n",
    "\n",
    "    if first_write:\n",
    "        merged.to_csv(path_out, index=False, sep=\";\", decimal=\".\", encoding=\"utf-8\")\n",
    "        first_write = False\n",
    "    else:\n",
    "        merged.to_csv(path_out, mode=\"a\", header=False, index=False, sep=\";\", decimal=\".\", encoding=\"utf-8\")\n",
    "\n",
    "orig_count = sum(1 for _ in open(path_val, encoding=\"utf-8\", errors=\"ignore\")) - 1\n",
    "\n",
    "print(f\"Filas originales en valores+weather: {orig_count:,}\")\n",
    "print(f\"Filas tras merge con malla        : {total_rows:,}\")\n",
    "\n",
    "\n",
    "final_count = sum(1 for _ in open(path_out, encoding=\"utf-8\", errors=\"ignore\")) - 1\n",
    "print(f\"Filas  escritas en salida: {final_count:,}\")\n",
    "\n",
    "assert orig_count == total_rows == final_count, \" Se ha perdido alguna fila.\"\n",
    "\n",
    "print(f\"\\n Merge completado correctamente; {final_count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b7e142-99c8-4aec-ab2e-ff25e3ae9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# junatr csv con todo + litologia \n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\Natascha\\Desktop\\Tesis\\pantallazos\\Nuevos_excel\")\n",
    "CSV_WEATHER = BASE / \"clima_bandas_coordenadas_dem_10_sin_nubes.csv\"    \n",
    "\n",
    "\n",
    "LITO_STEM = str(BASE / r\"Mapas\\SAM\\S2_lito_mineral_10000_2016_2025_sin_fechas\")\n",
    "LITO_CANDS = [LITO_STEM] if Path(LITO_STEM).exists() else sorted(glob(LITO_STEM + \"*\"))\n",
    "CSV_LITO = Path(LITO_CANDS[0])\n",
    "\n",
    "\n",
    "\n",
    "OUT_CSV  = BASE / \"weather_era5_join_lito_mineral_LEFT.csv\"     \n",
    "\n",
    "\n",
    "CHUNK     = 400_000                \n",
    "ENCODING  = \"utf-8\"\n",
    "SEP       = ';'\n",
    "NA_VALS   = [ \"NaN\"]\n",
    "\n",
    "\n",
    "def norm_id_series(s: pd.Series) -> pd.Series:\n",
    "    x = s.astype(str).str.strip()\n",
    "    as_num = pd.to_numeric(x, errors=\"coerce\")\n",
    "    m = as_num.notna()\n",
    "    x.loc[m] = as_num.loc[m].astype(\"Int64\").astype(str)\n",
    "    return x\n",
    "\n",
    "def norm_fecha_series(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def ensure_output_header(out_csv: Path, columns):\n",
    "    if out_csv.exists():\n",
    "        out_csv.unlink()\n",
    "    pd.DataFrame(columns=columns).to_csv(out_csv, index=False, sep=SEP, encoding=ENCODING)\n",
    "\n",
    "def fmt(n: Optional[int]) -> str:\n",
    "    return f\"{n:,}\" if isinstance(n, int) else \"?\"\n",
    "\n",
    "\n",
    "print(\"Construyendo índice de litología/mineral (numero, fecha_img)…\")\n",
    "lito_map: Dict[Tuple[str, str], Tuple[Optional[str], Optional[str]]] = {}\n",
    "rows_idx = 0\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "usecols_lito = [\"numero\", \"fecha_img\", \"litologia\", \"mineral\"]\n",
    "for i, chunk in enumerate(pd.read_csv(\n",
    "        CSV_LITO, chunksize=CHUNK, usecols=lambda c: c in usecols_lito,\n",
    "        encoding=ENCODING, sep=SEP, na_values=NA_VALS, dtype=str, engine=\"c\", low_memory=False\n",
    "    ), start=1):\n",
    "\n",
    "    if \"numero\" not in chunk.columns: chunk[\"numero\"] = pd.NA\n",
    "    if \"fecha_img\" not in chunk.columns: chunk[\"fecha_img\"] = pd.NA\n",
    "    chunk[\"numero\"]    = norm_id_series(chunk[\"numero\"])\n",
    "    chunk[\"fecha_img\"] = norm_fecha_series(chunk[\"fecha_img\"])\n",
    "\n",
    "\n",
    "    for c in [\"litologia\", \"mineral\"]:\n",
    "        if c not in chunk.columns:\n",
    "            chunk[c] = pd.NA\n",
    "\n",
    "\n",
    "    for key, lit, min_ in zip(\n",
    "        zip(chunk[\"numero\"], chunk[\"fecha_img\"]),\n",
    "        chunk[\"litologia\"], chunk[\"mineral\"]\n",
    "    ):\n",
    "\n",
    "        lito_map[key] = (lit, min_)\n",
    "\n",
    "    rows_idx += len(chunk)\n",
    "    if (i % 5) == 0:\n",
    "        dt = time.perf_counter() - t0\n",
    "        print(f\"  • Indexados {fmt(rows_idx)} registros LITO | {rows_idx/max(1e-9,dt):,.0f} filas/s\")\n",
    "\n",
    "print(f\"Filas LITO leídas: {fmt(rows_idx)}\")\n",
    "\n",
    "print(\"Uniendo sobre el CSV grande\")\n",
    "\n",
    "sample = pd.read_csv(CSV_WEATHER, nrows=5, encoding=ENCODING, sep=SEP, dtype=str, engine=\"c\")\n",
    "main_cols = list(sample.columns)\n",
    "\n",
    "\n",
    "main_cols_no_dup = [c for c in main_cols if c not in (\"litologia\", \"mineral\")]\n",
    "final_cols = main_cols_no_dup + [\"litologia\", \"mineral\"]\n",
    "ensure_output_header(OUT_CSV, final_cols)\n",
    "\n",
    "total_in = 0\n",
    "total_written = 0\n",
    "total_matches = 0\n",
    "printed_examples = 0\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "for j, chunk in enumerate(pd.read_csv(\n",
    "        CSV_WEATHER, chunksize=CHUNK, encoding=ENCODING,\n",
    "        sep=SEP, na_values=NA_VALS, dtype=str, engine=\"c\", low_memory=False\n",
    "    ), start=1):\n",
    "\n",
    "    for c in [\"punto_id\", \"fecha\"]:\n",
    "        if c not in chunk.columns:\n",
    "            raise ValueError(f\"Falta columna clave '{c}' en el CSV principal.\")\n",
    "\n",
    "    chunk[\"punto_id\"] = norm_id_series(chunk[\"punto_id\"])\n",
    "    chunk[\"fecha\"]    = norm_fecha_series(chunk[\"fecha\"])\n",
    "\n",
    "    for c in (\"litologia\", \"mineral\"):\n",
    "        if c in chunk.columns:\n",
    "            del chunk[c]\n",
    "\n",
    "\n",
    "    keys = pd.Series(list(zip(chunk[\"punto_id\"], chunk[\"fecha\"])), index=chunk.index)\n",
    "    mapped = keys.map(lito_map)  # Serie de tuplas o None\n",
    "\n",
    "    lit_ser = mapped.map(lambda v: v[0] if isinstance(v, tuple) else None)\n",
    "    min_ser = mapped.map(lambda v: v[1] if isinstance(v, tuple) else None)\n",
    "\n",
    "    matched_mask = lit_ser.notna() | min_ser.notna()\n",
    "    matches_chunk = int(matched_mask.sum())\n",
    "    total_matches += matches_chunk\n",
    "\n",
    "    out_chunk = pd.concat([chunk[main_cols_no_dup], lit_ser.rename(\"litologia\"), min_ser.rename(\"mineral\")], axis=1)\n",
    "    out_chunk = out_chunk[final_cols]\n",
    "    out_chunk.to_csv(OUT_CSV, mode=\"a\", index=False, header=False, sep=SEP, encoding=ENCODING)\n",
    "\n",
    "    total_in += len(chunk)\n",
    "    total_written += len(out_chunk)\n",
    "\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"[{j:>4}] {len(out_chunk):,} filas | matches {matches_chunk:,} | \"\n",
    "          f\"acum {total_written:,} | {total_written/max(1e-9,dt):,.0f} filas/s | {dt:0.1f}s\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n──────── Resumen ────────\")\n",
    "print(f\"Filas entrada (CSV grande): {fmt(total_in)}\")\n",
    "print(f\"Filas salida (LEFT join):   {fmt(total_written)}  (debe ser == a entrada)\")\n",
    "print(f\"Filas con match LITO:       {fmt(total_matches)}\")\n",
    "print(f\"Archivo de salida: {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f6d0a-406d-4ad0-b0f5-79606d0c0dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d771c0-36f3-4ebf-b63f-7fb711e6dc65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c180e47-5f89-4963-9243-41d59b6fbb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2627d-5250-4c1e-9cd7-6ad28e8c5eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea600c-32fd-46bc-b5ae-ab0fb525dcb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5353a67a-8178-4afa-b52f-d65eabdb2019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18322845-2476-4d83-b12c-632f775ee16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01125d-b3fe-40cc-a595-da67ecd2f407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715ae60-14e7-4fc8-a7a1-d0b242ac4499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a889425-63c2-4072-a5aa-cabc7a66b884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159115f-b9ab-46cd-82db-e520e95d488d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e740a6-6fb2-485d-b716-0ca08799d062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
